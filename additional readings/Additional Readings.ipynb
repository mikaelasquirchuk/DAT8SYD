{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Readings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* An Introduction to Gradient Descent and Linear Regression, Matt Nedrich: https://spin.atomicobject.com/2014/06/24/gradient-descent-linear-regression/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Introduction to Gradient Descent and Linear Regression, Matt Nedrich:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "* The goal of linear regression is to fit a line to a set of data points.\n",
    "* The line will be y = mx - b (m being the gradient; b being the y-intercept)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "y = mx - b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We want to find the best m and b to fit our data. \n",
    "\n",
    "### Error functions\n",
    "* One way to solve this problem is to define an error or cost function that measures how good a line is.\n",
    "* This function will take in a (m,b) pair and return an error value based on how well the line fits our data. i.e. Error (m,b) = ...\n",
    "* To find the error for a line called l, you would go through each x point in the data set and sum the square distances between each point's y value and l's y value. \n",
    "* We square the distance to ensure it is positive and to make our error function differentiable - so that points that are below the line being tested don't cancel out points above the line.\n",
    "* In python, computing the error for a given line will look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# y = mx + b\n",
    "# m is slope, b is y-intercept\n",
    "def computeErrorForLineGivenPoints(b, m, points):\n",
    "    totalError = 0\n",
    "    for i in range(0, len(points)):\n",
    "        totalError += (points[i].y - (m * points[i].x + b)) ** 2\n",
    "    return totalError / float(len(points))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Formally, this error function looks like:\n",
    "![Forumula for Linear Regression Error](https://spin.atomicobject.com/wp-content/uploads/linear_regression_error1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The line that has the smallest error value will be our best line. \n",
    "* Since our error function consists of two parameters (m and b) we can visualize it as a two-dimensional surface. This is what it looks like for our data set:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![error function](https://spin.atomicobject.com/wp-content/uploads/gradient_descent_error_surface.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Each point in the 2-dimensional axis represents a line (y = mx + b). The height of the function at each point is the error value for that line.\n",
    "* Some lines yield smaller errors than others.\n",
    "\n",
    "### Gradient descent search\n",
    "* *Gradient descent search* starts from some location at this surface and tests lines oving downhill to find the line with the lowest error.\n",
    "* To run gradient descent on the Error function above, we need to determine its gradient by differentiating.\n",
    "* With two parameters, we will need partial derivatives for each: \n",
    "![Forumula for Linear Regression Error](https://spin.atomicobject.com/wp-content/uploads/linear_regression_gradient1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The search starts at any line and then the gradient descent algorith continues until we find the best line (with the lowest error, where the gradient is closes to zero). \n",
    "* Each time m and be will be updated to a line with a lower error.\n",
    "* The direction to move in is calculated using the two partial derivativesEach iteration will update m and b to a line that yields slightly lower error than the previous iteration. The direction to move in for each iteration is calculated using the two partial derivatives from above and looks like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stepGradient(b_current, m_current, points, learningRate):\n",
    "    b_gradient = 0\n",
    "    m_gradient = 0\n",
    "    N = float(len(points))\n",
    "    for i in range(0, len(points)):\n",
    "        b_gradient += -(2/N) * (points[i].y - ((m_current*points[i].x) + b_current))\n",
    "        m_gradient += -(2/N) * points[i].x * (points[i].y - ((m_current * points[i].x) + b_current))\n",
    "    new_b = b_current - (learningRate * b_gradient)\n",
    "    new_m = m_current - (learningRate * m_gradient)\n",
    "    return [new_b, new_m]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning rate\n",
    "* The learningRate variable controls how big the step between lines (downhill) during each iteration. Bigger steps mean we may step over the minimum. Smaller steps require more iterations.\n",
    "\n",
    "#### Visualising the gradient descent search\n",
    "* In the below gradient descent search, the search starts at y = -x, and it shifts each iteration for a better fit for 2000 iterations. It ends with an accurate fit. It also shows (on the left) how m and b change as the line moves towards the minimum.\n",
    "\n",
    "![Iterations of Gradient Descent](https://spin.atomicobject.com/wp-content/uploads/gradient_descent_search1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Check how the error changes as it looks for the best line. \n",
    "* The line should go down!\n",
    "![Gradient Descent error](https://spin.atomicobject.com/wp-content/uploads/gradient_descent_error_by_iteration.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This way of minimising an error function applies to linear models, but also to problems that use higher order polynomials, and other problems.\n",
    "\n",
    "#### Additional Concepts\n",
    "* Additional concepts to keep in mind are:\n",
    " * _Convexity_ – There's only one minimum error in our linear regression problem because the error surface area is convex. In general, this need not be the case. It’s possible to have a problem with local minima that a gradient search can get stuck in. There are several approaches to mitigate this (e.g., stochastic gradient search).\n",
    " * _Performance_ – We used vanilla gradient descent with a learning rate of 0.0005 in the above example, and ran it for 2000 iterations. There are approaches such a line search, that can reduce the number of iterations required. For the above example, line search reduces the number of iterations to arrive at a reasonable solution from several thousand to around 50.\n",
    "* Convergence – We didn’t talk about how to determine when the search finds a solution. This is typically done by looking for small changes in error iteration-to-iteration (e.g., where the gradient is near zero)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Code\n",
    "* Example code for the problem described above can be found here: https://github.com/mattnedrich/GradientDescentExample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Link to the article here](https://spin.atomicobject.com/2014/06/24/gradient-descent-linear-regression/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
