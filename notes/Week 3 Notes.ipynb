{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 3 Lesson 1\n",
    "## Logistic Regression\n",
    "5 - 6 June 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression vs Classification\n",
    "* If the y variable is numeric then we have a regression problem - we are trying to predict a continuous number\n",
    "* If the y variable is a category (for example trying to predict a type of flower) the we have a classification problem - we are trying to classify what group that y belongs to.\n",
    "* We want to build a classifier that correctly identifies which class our target variable y belongs to given our input variable x. \n",
    "* Why not use the linear regression model? y=Xβ+ϵ\n",
    " * If we only have a binary response variable (0 or 1) it might make sense… BUT we can have our estimated value of y > 1 or y < 0 … which doesn’t make sense.\n",
    " * What of the case where we have more than one class? Linear regression cannot easily handle these cases.\n",
    " * We want a classification method that can handle these cases and give us results we can easily interpret. \n",
    "\n",
    "<img src=\"img/logit-function.png\" width=400 align=right>\n",
    "### p(1|x) = β0+β1\n",
    "* This is a good starting point but we still have the problem of p(Y) being outside the 0,1 range.\n",
    "* We need to model p(Y=1|X) using a function that gives outputs between 0 and 1. Basically we want something that looks like the following:\n",
    "<img src=\"img/logistic-regression.png\" width=300 align=right>\n",
    "\n",
    "\n",
    "\n",
    "* We can see that it this function is linear in X\n",
    " * p | (p - 1) is called the ‘odds’ and can be any value from 0 to ∞\n",
    " * log ( p | (p - 1) ) is called the ‘log-odds’ or ‘logit’"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Score\n",
    "* This is simply the fraction of correct predictions from the model. So it is the number of correct predictions divided by the number of observations in our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/confusion-matrix.png\" align=right>\n",
    "### Confusion Matrix\n",
    "* A confusion matrix shows us what the predicted class was against what the actual class was. \n",
    "* The true class makes up the rows or the vertical axes and the predicted class makes up the columns or the horizontal axis.\n",
    "* Any entries in the diagonal of the matrix are those that are correctly classified. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/roc-curve.png\" align=right>\n",
    "### Receiver Operating Characteristic\n",
    "\n",
    "* The Receiver Operating Characteristic or ROC curve shows the performance of a binary classifier system as its discrimination threshold is varied. \n",
    "* It is created by plotting the fraction of true positives out of the positives (TPR = true positive rate) vs. the fraction of false positives out of the negatives (FPR = false positive rate), at various threshold settings.\n",
    "* By computing the Area Under the Curve of the ROC curve we get a single number summary of accuracy.\n",
    "* The closer that number is to 1 the more accurate our model is. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#read the data\n",
    "titanic = pd.read_csv('../data/titanic.csv', index_col='PassengerId')\n",
    "#define the columns to read (x), and the target (y)\n",
    "feature_cols = ['Pclass', 'Parch']\n",
    "X = titanic[feature_cols]\n",
    "y = titanic.Survived\n",
    "#split the data\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "#fit the logreg model and print the coefficients\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "zip(feature_cols, logreg.coef_[0])\n",
    "print(logreg.fit(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.668161434978\n",
      "[[105  23]\n",
      " [ 51  44]]\n"
     ]
    }
   ],
   "source": [
    "#make predictions on testing set and test accuracy\n",
    "y_pred_class = logreg.predict(X_test)\n",
    "from sklearn import metrics\n",
    "print(metrics.accuracy_score(y_test, y_pred_class))\n",
    "#print the confusion matrix\n",
    "from sklearn import metrics\n",
    "prds = logreg.predict(X)\n",
    "print(metrics.confusion_matrix(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#generate the ROC curve\n",
    "import matplotlib.pyplot as plt\n",
    "# Generate the prediction values for each of the test observations using predict_proba() function rather than just predict\n",
    "preds = logreg.predict_proba(X_test)[:,1]\n",
    "# Store the false positive rate(fpr), true positive rate (tpr) in vectors for use in the graph\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test, preds)\n",
    "# Store the Area Under the Curve (AUC) so we can annotate our graph with this metric\n",
    "roc_auc = metrics.auc(fpr,tpr)\n",
    "# Plot the ROC Curve\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "# What's happening here is we are changing the cutoff value from 0 to 1.\n",
    "# When we have a cutoff of zero this means that we have no positive predictions so both fpr and tpr are both 0\n",
    "# Our aim when modelling is to maximise the area under the curve, the closer to one the better the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#split data into binaries where not binary\n",
    "titanic_with_dummies = pd.get_dummies(data=titanic, columns = ['Sex', 'Embarked', 'Pclass'], prefix = ['Sex', 'Embarked', 'Pclass'] )\n",
    "titanic_with_dummies['Age'] = titanic_with_dummies[['Age',\"Parch\",\"Sex_male\",'Pclass_1', 'Pclass_2']].groupby([\"Parch\",\"Sex_male\",'Pclass_1', 'Pclass_2'])['Age'].transform(lambda x: x.fillna(x.mean()))\n",
    "feature_cols = ['Parch', 'Sex_male', 'Sex_female', 'Pclass_1', 'Pclass_2', 'Pclass_3', 'Fare', 'Age']\n",
    "X = titanic_with_dummies[feature_cols]\n",
    "y = titanic_with_dummies.Survived\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "logreg.fit(X_train, y_train)\n",
    "zip(feature_cols, logreg.coef_[0])\n",
    "y_pred_class = logreg.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred_class))\n",
    "preds = logreg.predict_proba(X_test)[:,1]\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test, preds)\n",
    "roc_auc = metrics.auc(fpr,tpr)\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "KNN_model = KNeighborsClassifier(5)\n",
    "KNN_model.fit(X_train, y_train)\n",
    "y_pred_class = KNN_model.predict(X_test)\n",
    "# Print the new accuracy rate\n",
    "print(metrics.accuracy_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "NB_model = GaussianNB()\n",
    "NB_model.fit(X_train, y_train)\n",
    "y_pred_class = NB_model.predict(X_test)\n",
    "# Print the new accuracy rate\n",
    "print(metrics.accuracy_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional readings\n",
    "* Logistic Regression applied to loan applications https://github.com/nborwankar/LearnDataScience\n",
    "* Odds Ratio in Logistic Regression http://www.ats.ucla.edu/stat/mult_pkg/faq/general/odds_ratio.htm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 3 Lesson 2\n",
    "## Model Evaluation\n",
    "6 - 8 June 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is model evaluation?\n",
    "* Model evaluation is how we test whether our model works on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why do we do model evaluation?\n",
    "* A model bsed on training data has a set error - the training error - for how well the model fits training data. This is a good way to see how the model fits _that_ data, but not all data. \n",
    "* We want to test other data on the model for a better measure of accuracy.\n",
    "* Without model evaluation, you run the risk of over-fitting the training data, which will have high accuracy for the training data but low accuracy for everything else.\n",
    "* This is what we call **bias**: if your model has too much bias, it will be accurate for the training data, but will be too closely fit to the data and so won't have the same accuracy for new data.\n",
    "* **Variance** is the opposite: if your model has too much variance, it won't be accurate for the training data, because it's not fit well enough.\n",
    "<img src=\"img/bias-and-variance.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### High Variance, Low Bias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_table('http://people.sc.fsu.edu/~jburkardt/datasets/regression/x01.txt', sep='\\s+', skiprows=33, names=['id','brain','body'], index_col='id')\n",
    "np.random.seed(12345)\n",
    "df = df[df.body < 200]\n",
    "df['sample'] = np.random.randint(1, 3, len(df))\n",
    "\n",
    "sns.lmplot(x='body', y='brain', data=df, ci=None, hue='sample')\n",
    "sns.plt.xlim(-10, 200)\n",
    "sns.plt.ylim(-10, 250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Low Variance, High Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.lmplot(x='body', y='brain', data=df, ci=None, hue='sample', order=8)\n",
    "sns.plt.xlim(-10, 200)\n",
    "sns.plt.ylim(-10, 250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Good balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.lmplot(x='body', y='brain', data=df, ci=None, hue='sample', order=2)\n",
    "sns.plt.xlim(-10, 200)\n",
    "sns.plt.ylim(-10, 250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we do model evaluation?\n",
    "* We do model evaluation by separating out training data and test data from our dataset, and using the training data to build the model. We then test its accuracy on test data. This is the out-of-sample error (OOS).\n",
    "* If we had a different training/test split, the OOS error would be different.\n",
    "* One way of improving this will be to use multiple splits for test and training data. Train the model with different splits of data, and test with the rest from that split.\n",
    "* This is called **cross-validation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#We'll be using logistic regression with Titanic dataset:\n",
    "import pandas as pd\n",
    "titanic = pd.read_csv('../data/titanic.csv', index_col='PassengerId')\n",
    "#Imputing values for Age:\n",
    "from sklearn.preprocessing import Imputer\n",
    "imp = Imputer(strategy='mean', axis=1)\n",
    "titanic['Age'] = imp.fit_transform(titanic.Age.values.reshape(1,-1)).T\n",
    "#Logreg on Age:\n",
    "feature_cols = ['Pclass', 'Parch','Age']\n",
    "X = titanic[feature_cols]\n",
    "y = titanic.Survived\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(C=1e9)\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred_class = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-validation\n",
    "* Steps for K-fold cross-validation:\n",
    " 1. Randomly split the dataset into K equal partitions.\n",
    " 2. Use partition 1 as test set & union of other partitions as training set.\n",
    " 3. Calculate test set error.\n",
    " 4. Repeat steps 2-3 using a different partition as the test set at each iteration.\n",
    " 5. Take the average test set error as the estimate of OOS accuracy.\n",
    "* Features of K-fold cross-validation:\n",
    " 1. More accurate estimate of OOS prediction error.\n",
    " 2. More efficient use of data than single train/test split.\n",
    "   * Each record in our dataset is used for both training and testing.\n",
    " 3. Presents tradeoff between efficiency an computational expense.\n",
    "   * 10-fold CV is 10x more expensive than a single train/test split\n",
    " 4. Can be used for parameter tuning and model selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titanicwd = pd.get_dummies(data=titanic, columns = ['Sex', 'Embarked', 'Pclass'], prefix = ['Sex', 'Embarked', 'Pclass'] )\n",
    "# Fill any null values of age by taking average by gender, class and parch\n",
    "titanicwd['Age']=titanicwd[[\"Age\",\"Parch\",\"Sex_male\",\"Pclass_1\",\"Pclass_2\"]].groupby([\"Parch\",\"Sex_male\",'Pclass_1', 'Pclass_2'])['Age'].transform(lambda x: x.fillna(x.mean()))\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "feature_cols = ['Sex_male', 'Sex_female','Embarked_C','Embarked_Q','Embarked_S','Pclass_1', 'Pclass_2', 'Pclass_3','Parch', 'Age','SibSp']\n",
    "X = titanicwd[feature_cols]\n",
    "y = titanicwd.Survived\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "logreg.fit(X_train, y_train)\n",
    "zip(feature_cols, logreg.coef_[0])\n",
    "y_pred_class = logreg.predict(X_test)\n",
    "\n",
    "print(metrics.accuracy_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scores = cross_val_score(LogisticRegression(), X, y, scoring='accuracy', cv=10)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import cross_validation\n",
    "from sklearn import linear_model\n",
    "mse_values = []\n",
    "scores = []\n",
    "n = 0\n",
    "\n",
    "feature_cols = ['Sex_male', 'Sex_female','Embarked_C','Embarked_Q','Embarked_S','Pclass_1', 'Pclass_2', 'Pclass_3',]\n",
    "modeldata = titanicwd[feature_cols]\n",
    "y = titanicwd.Survived \n",
    "\n",
    "kf = cross_validation.KFold(len(modeldata), n_folds=10, shuffle=True)\n",
    "\n",
    "print(\"~~~~ CROSS VALIDATION each fold ~~~~\")\n",
    "for train_index, test_index in kf:\n",
    "    lm = linear_model.LinearRegression().fit(modeldata.iloc[train_index], y.iloc[train_index])\n",
    "    mse_values.append(metrics.mean_squared_error(y.iloc[test_index], lm.predict(modeldata.iloc[test_index])))\n",
    "    scores.append(lm.score(modeldata, y))\n",
    "    n+=1\n",
    "    print ('Model', n)\n",
    "    print ('MSE:', mse_values[n-1])\n",
    "    print ('R2:', scores[n-1])\n",
    "\n",
    "\n",
    "print (\"~~~~ SUMMARY OF CROSS VALIDATION ~~~~\")\n",
    "print ('Mean of MSE for all folds:', np.mean(mse_values))\n",
    "print ('Mean of R2 for all folds:', np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Null accuracy\n",
    "* **Null accuracy** is the accuracy that could be achieved by always predicting the **most frequent class**. It is a baseline against which you may want to measure your classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (y_test.mean())\n",
    "print (1 - y_test.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "dumb = DummyClassifier(strategy='most_frequent')\n",
    "dumb.fit(X_train, y_train)\n",
    "y_dumb_class = dumb.predict(X_test)\n",
    "print (metrics.accuracy_score(y_test, y_dumb_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion matrix\n",
    "* Confusion Matrix is another way to evaluate a model. It is a table to describe the performance of a classifier.\n",
    "* **Sensitivity | Recall** = (number of true positives) | (true positives + false negatives - as in, number of all real positives)\n",
    "* **Specificity** = (number of true negatives) | (true negatives + false positives - as in number of all real negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "metrics.confusion_matrix(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load confusion_matrix_nice.py\n",
    "#from confusion_matrix_nice import plot_confusion_matrix\n",
    "%run confusion_matrix_nice\n",
    "cnf_mat = metrics.confusion_matrix(y_test, y_pred_class, labels = titanic.Survived.unique())\n",
    "class_labels = titanic.Survived.unique()\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_mat, class_labels,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sensitivity = TP / TP + FN\n",
    "#specificity = TN / TN + FP\n",
    "sensitivity=43/(43+52)\n",
    "specificity = 107/(107+21)\n",
    "print(sensitivity,specificity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred_prob = logreg.predict_proba(X_test)[:, 1]\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(y_pred_prob)\n",
    "plt.xlabel('Predicted probability of survival')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "y_pred_class = np.where(y_pred_prob > 0.25, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (metrics.confusion_matrix(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sensitivity = 68 / (27 + 68)\n",
    "specificity = 57/ (57 + 71)\n",
    "print(sensitivity,specificity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision + recall\n",
    "* **Precision** = (number of true positives) | (true positives + false positives - as in number of all predicted positives)\n",
    "* **Sensitivity | Recall** = (number of true positives) | (true positives + false negatives - as in, number of all real positives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Accuracy score returns the percent of true positives and true negatives samples (in decimal format with 1 being perfect)\n",
    "print('Accuracy score:',metrics.accuracy_score(y_test, y_pred_class))\n",
    "#Precision score returns the correctly identified \n",
    "# = # true positives / true positives + false positives:\n",
    "print('Precision score:',metrics.precision_score(y_test, y_pred_class))\n",
    "#Recall score returns the # of  positives correctly predicted \n",
    "# = # true positives / true positives + false negatives:\n",
    "print('Recall score:',metrics.recall_score(y_test,y_pred_class))\n",
    "# The F1 score can be interpreted as a weighted average of the precision and recall.\n",
    "# An F1 score reaches its best value at 1 and worst score at 0. \n",
    "# The relative contribution of precision and recall to the F1 score are equal. \n",
    "# F1 = 2 * (precision * recall) / (precision + recall)\n",
    "print('F1 score:',metrics.f1_score(y_test,y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional readings\n",
    "* Caltech's Learning From Data course visualising bias and variance (15 mins) http://work.caltech.edu/library/081.html\n",
    "* Rahul Patwari has a great video on ROC Curves (12 minutes)  https://www.youtube.com/watch?v=21Igj5Pr6u4\n",
    "* Have a look at scikit-learn’s documentation on model evaluation http://scikit-learn.org/stable/modules/model_evaluation.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
