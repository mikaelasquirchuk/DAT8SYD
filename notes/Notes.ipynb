{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "- <a href=\"#Week-1-Lesson-1\">Week 1 Lesson 1: Welcome + course overview</a>\n",
    "  - 1 - 23 May 2017\n",
    "- <a href=\"#Week-1-Lesson-2\">Week 1 Lesson 2: Git + Github, Numpy + Pandas</a>\n",
    "  - 2 - 25 May 2017\n",
    "- <a href=\"#Week-2-Lesson-1\">Week 2 Lesson 1: Visualisation</a>\n",
    "  - 3 - 30 May 2017\n",
    "- <a href=\"#Week-2-Lesson-2\">Week 2 Lesson 2: Linear Regression</a>\n",
    "  - 4 - 1 June 2017\n",
    "- <a href=\"#Week-3-Lesson-1\">Week 3 Lesson 1: Logistic Regression</a>\n",
    "  - 5 - 6 June 2017\n",
    "- <a href=\"#Week-3-Lesson-2\">Week 3 Lesson 2: Model Evaluation</a>\n",
    "  - 6 - 8 June 2017\n",
    "- <a href=\"#Week-4-Lesson-1\">Week 4 Lesson 1: Regularisation</a>\n",
    "  - 7 - 13 June 2017\n",
    "- <a href=\"#Week-4-Lesson-2\">Week 4 Lesson 2: Clustering</a>\n",
    "  - 8 - 15 June 2017\n",
    "- <a href=\"#Week-5-Lesson-1\">Week 5 Lesson 1: Recommendation Engines</a>\n",
    "  - 9 - 20 June 2017\n",
    "- <a href=\"#Week-5-Lesson-2\">Week 5 Lesson 2: Databases + SQL in Python</a>\n",
    "  - 10 - 22 June 2017\n",
    "- <a href=\"#Week-6-Lesson-1\">Week 6 Lesson 1: Decision trees</a>\n",
    "  - 11 - 27 June 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 1 Lesson 1\n",
    "## Welcome + course overview\n",
    "1 - 23 May 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Course overview\n",
    "* First, get a basic level of all the different areas of Data Science - theory, programming, statistics, visualisation, communication, subject matter expertise.\n",
    "* Then get to go into further detail in more advanced areas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional readings\n",
    "* Python for Data Analysis:  http://shop.oreilly.com/product/0636920023784.do\n",
    "* Command Line Crash Course: http://cli.learncodethehardway.org/book/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 1 Lesson 2\n",
    "## Git + Github, Numpy + Pandas\n",
    "2 - 25 May 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Git\n",
    "* Version control system that allows you to track files and file changes in a repository (“repo”)\n",
    "* Primarily used by software developers\n",
    "* Most widely used version control system\n",
    "* Alternatives: Mercurial, Subversion, CVS\n",
    "* Runs from the command line (usually)\n",
    "* Can be used alone or in a team\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Github\n",
    "* Allows you to put your Git repos online\n",
    "* Alternative: Bitbucket\n",
    "* Benefits of GitHub:\n",
    " * Backup of files\n",
    " * Visual interface for navigating repos\n",
    " * Makes repo collaboration easy\n",
    "* Git does not require GitHub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=git_chart.png width=500 align=left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy\n",
    "\n",
    "* Numpy can be used to perform general maths functions, and also create arrays of data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = np.array( [20,30,40,50] )\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b = np.arange(4)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = a-b\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = b*2\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "e = np.random.randint(1,10,(2,3,4))\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "e[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas\n",
    "* Pandas is all about that reading. You can read files using pandas, including CSV files, URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.read_table('../data/u.user', header=None)\n",
    "user_cols = ['user_id', 'age', 'gender', 'occupation', 'zip_code']\n",
    "users = pd.read_table('../data/u.user', sep='|', header=None, names=user_cols, index_col='user_id', dtype={'zip_code':str})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "users                   # print the first 30 and last 30 rows\n",
    "type(users)             # DataFrame\n",
    "users.head()            # print the first 5 rows\n",
    "users.head(10)          # print the first 10 rows\n",
    "users.tail()            # print the last 5 rows\n",
    "users.index             # \"the index\" (aka \"the labels\")\n",
    "users.columns           # column names (which is \"an index\")\n",
    "users.dtypes            # data types of each column\n",
    "users.shape             # number of rows and columns\n",
    "users.values            # underlying numpy array\n",
    "users.info()            # concise summary (including memory usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#users['gender']         # select one column\n",
    "#type(users['gender'])   # Series\n",
    "#users[['gender']]\n",
    "#type(users[['gender']])   # DataFrame\n",
    "#users.gender            # select one column using the DataFrame attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#users.describe()                    # describe all numeric columns\n",
    "#users.describe(include=['object'])  # describe all object columns (can include multiple types)\n",
    "#users.describe(include='all')       # describe all columns\n",
    "#users.gender.describe()             # describe a single column\n",
    "users.age.mean()                    # only calculate the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#users.occupation.value_counts()     # most useful for categorical variables\n",
    "users.age.value_counts()        # can also be used with numeric variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#young_bool = users.age < 20         # create a Series of booleans...\n",
    "#users[young_bool]                   # ...and use that Series to filter rows\n",
    "#users[users.age < 20]               # or, combine into a single step\n",
    "#users[users.age < 20].occupation    # select one column from the filtered results\n",
    "users[users.age < 20].occupation.value_counts()     # value_counts of resulting Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# logical filtering with multiple conditions\n",
    "#users[(users.age < 20) & (users.gender=='M')]       # ampersand for AND condition\n",
    "#users[(users.age < 20) | (users.age > 60)]          # pipe for OR condition\n",
    "users[users.occupation.isin(['doctor', 'lawyer'])]  # alternative to multiple OR conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "users.age.sort_values()                   # sort a column\n",
    "users.sort_values(by='age')                   # sort a DataFrame by a single column\n",
    "users.sort_values(by='age', ascending=False)  # use descending order instead\n",
    "users.sort_values(by=['occupation', 'age'])   # sort by multiple columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing values\n",
    "scikit-learn models expect that all values are numeric and hold meaning. Thus, missing values are not allowed by scikit-learn. One possible strategy is to just drop missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "users.age.value_counts()              # excludes missing values\n",
    "users.age.value_counts(dropna=False)  # includes missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "users.age.isnull()           # True if missing, False if not missing\n",
    "#users.age.isnull().sum()     # count the missing values\n",
    "#users.age.notnull()          # True if not missing, False if missing\n",
    "#users[users.age.notnull()]  # only show rows where continent is not missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use 'tilde' ~ to negate the boolean values\n",
    "~users.age.isnull()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "users.isnull()             # DataFrame of booleans\n",
    "users.isnull().sum()       # count the missing values in each column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way is to impute missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "users.age.fillna(value='NA')                 # fill in missing values with 'NA'\n",
    "users.age.fillna(value='NA', inplace=True)   # modifies 'drinks' in-place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "users.sum(axis=0)      # sums \"down\" the 0 axis (rows)\n",
    "users.sum()            # axis=0 is the default\n",
    "users.sum(axis=1)      # sums \"across\" the 1 axis (columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional readings\n",
    "[Basic Git commands list](\"https://confluence.atlassian.com/bitbucketserver/basic-git-commands-776639767.html\")\n",
    "\n",
    "[Good resourses](\"https://help.github.com/articles/git-and-github-learning-resources/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 2 Lesson 1\n",
    "## Visualisation\n",
    "3 - 30 May 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "import matplotlib.pyplot as plt\n",
    "#!pip install seaborn\n",
    "import seaborn as sns\n",
    "#!pip install seaborn\n",
    "import plotly\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go \n",
    "#!pip install folium\n",
    "import folium\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "plotly.tools.set_credentials_file(username='Msquirchuk', api_key='kTj4gydNrsbGuMDjMcNn')\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(color_codes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is data visualisation?\n",
    "* Data visualisation is the representation of data in a graphical way to more easily or clearly convey the patterns in data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why do we visualise data?\n",
    "* Visualisation helps data analysts:\n",
    " * understand the data and what patterns exist (exploratory);\n",
    " * explain the data to non-data-ists (reporting) or in a simple way.\n",
    "* The greatest value of a picture is when it forces us to notice what we never expected to see. - John W Tukey, Exploratory Data Analysis, 1977."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How can we visualise data?\n",
    "* There are many data visualisation tools: the easier ones to use can do less stuff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pythonvislandscape.png\" width=450 align=left>\n",
    "<img src=\"chart-suggestions.png\" width=450 align=right>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we visualise data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bar charts\n",
    "* Shows numeric summaries across different categories (either horizontally or vertically)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = [3, 10, 7, 5, 3, 4.5, 6, 8.1]\n",
    "N = len(y)\n",
    "x = range(N)\n",
    "plt.bar(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zoodata = [go.Bar(\n",
    " x=['giraffes', 'orangutans', 'monkeys'],\n",
    " y=[20, 14, 23]\n",
    " )]\n",
    "\n",
    "py.iplot(zoodata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://raw.githubusercontent.com/plotly/datasets/master/school_earnings.csv\")\n",
    "\n",
    "trace_women = Bar(x=df.School,\n",
    "                  y=df.Women,\n",
    "                  name='Women',\n",
    "                  marker=dict(color='#ffcdd2'))\n",
    "\n",
    "trace_men = Bar(x=df.School,\n",
    "                y=df.Men,\n",
    "                name='Men',\n",
    "                marker=dict(color='#A2D5F2'))\n",
    "\n",
    "trace_gap = Bar(x=df.School,\n",
    "                y=df.gap,\n",
    "                name='Gap',\n",
    "                marker=dict(color='#59606D'))\n",
    "\n",
    "data = [trace_women, trace_men, trace_gap]\n",
    "layout = Layout(title=\"Average Earnings for Graduates\",\n",
    "                xaxis=dict(title='School'),\n",
    "                yaxis=dict(title='Salary (in thousands)'))\n",
    "\n",
    "fig = Figure(data=data, layout=layout)\n",
    "\n",
    "py.iplot(fig, filename='styled_bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histograms\n",
    "* Shows the distribution of data over a continuous interval\n",
    "* Allows us to see the shape of our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_data=urllib.request.urlopen(\"http://goo.gl/HppjFh\")\n",
    "dataset = pd.read_csv(raw_data, delimiter=\",\", names=('sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species'))\n",
    "dataset[['sepal_width']].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset.loc[dataset['species'] == 'Iris-setosa','sepal_length'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.random.randn(500)\n",
    "data = [go.Histogram(x=x)]\n",
    "py.iplot(data, filename='basic histogram') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scatterplots\n",
    "* Shows values between two variables, one on each axis.\n",
    "* Used to see a relationship between variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(dataset['sepal_length'],dataset['sepal_width'],100,'rgb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.lmplot(x='sepal_width', y=\"sepal_length\", hue=\"species\", data=dataset, fit_reg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.jointplot(x=\"sepal_width\", y=\"sepal_length\", data=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.pairplot(dataset, hue='species');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = 1000\n",
    "random_x = np.random.randn(N)\n",
    "random_y = np.random.randn(N)\n",
    "trace = go.Scatter(\n",
    " x = random_x,\n",
    " y = random_y,\n",
    " mode = 'markers'\n",
    ")\n",
    "data = [trace]\n",
    "py.iplot(data, filename='basic-scatter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Box and whisker plots\n",
    "* Displays numerical distribution summaries by groups through quartiles\n",
    "* Can compare different distributions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spread = np.random.rand(50) * 100\n",
    "center = np.ones(25) * 50\n",
    "flier_high = np.random.rand(10) * 100 + 100\n",
    "flier_low = np.random.rand(10) * -100\n",
    "data = np.concatenate((spread, center, flier_high, flier_low), 0)\n",
    "\n",
    "plt.boxplot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spread = np.random.rand(50) * 100\n",
    "center = np.ones(25) * 40\n",
    "flier_high = np.random.rand(10) * 100 + 100\n",
    "flier_low = np.random.rand(10) * -100\n",
    "d2 = np.concatenate((spread, center, flier_high, flier_low), 0)\n",
    "data.shape = (-1, 1)\n",
    "d2.shape = (-1, 1)\n",
    "# data = concatenate( (data, d2), 1 )\n",
    "# Making a 2-D array only works if all the columns are the\n",
    "# same length.  If they are not, then use a list instead.\n",
    "# This is actually more efficient because boxplot converts\n",
    "# a 2-D array into a list of vectors internally anyway.\n",
    "data = [data, d2, d2[::2, 0]]\n",
    "# multiple box plots on one figure\n",
    "plt.figure()\n",
    "plt.boxplot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y0 = np.random.randn(50)-1\n",
    "y1 = np.random.randn(50)+1\n",
    "trace0 = go.Box(\n",
    " y=y0\n",
    ")\n",
    "trace1 = go.Box(\n",
    " y=y1\n",
    ")\n",
    "data = [trace0, trace1]\n",
    "py.iplot(data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Density plots\n",
    "* Similar to a histogram but smooths out the distribution with a continuous line\n",
    "* Not affected by bin choices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "data = [1.5]*7 + [2.5]*2 + [3.5]*8 + [4.5]*3 + [5.5]*1 + [6.5]*8\n",
    "\n",
    "density = gaussian_kde(data)\n",
    "xs = np.linspace(0,8,200)\n",
    "density.covariance_factor = lambda : .25\n",
    "density._compute_covariance()\n",
    "plt.plot(xs,density(xs))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heat maps\n",
    "* Colour coding applied to tabular data.\n",
    "* Provides a generalised view of the data by each cell "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.random.randn(8873)\n",
    "y = np.random.randn(8873)\n",
    "\n",
    "heatmap, xedges, yedges = np.histogram2d(x, y, bins=50)\n",
    "extent = [xedges[0], xedges[-1], yedges[0], yedges[-1]]\n",
    "\n",
    "plt.clf()\n",
    "plt.imshow(heatmap.T, extent=extent, origin='lower')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trace = go.Heatmap(z=[[1, 20, 30],\n",
    " [20, 1, 60],\n",
    " [30, 60, 1]])\n",
    "data=[trace]\n",
    "py.iplot(data, filename='basic-heatmap') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.random.randn(2000)\n",
    "y = np.random.randn(2000)\n",
    "plotly.offline.iplot([Histogram2dContour(x=x, y=y, contours=Contours(coloring='heatmap')),\n",
    "       Scatter(x=x, y=y, mode='markers', marker=Marker(color='white', size=3, opacity=0.3))], show_link=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Line graphs\n",
    "* Used to display a numeric value over a continuous value or time\n",
    "* Used to observe trends and changes over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = range(3)\n",
    "x = [3,4,5]\n",
    "plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = 500\n",
    "random_x = np.linspace(0, 1, N)\n",
    "random_y = np.random.randn(N)\n",
    "trace = go.Scatter(\n",
    " x = random_x,\n",
    " y = random_y\n",
    ")\n",
    "data = [trace]\n",
    "py.iplot(data, filename='basic-line') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parallel coordinates\n",
    "* Plot multiple numeric variables across each observation\n",
    "* Each axis is scaled and each line through the graph is an observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    go.Parcoords(\n",
    "        line = dict(color = 'blue'),\n",
    "        dimensions = list([\n",
    "            dict(range = [1,5],\n",
    "                 constraintrange = [1,2],\n",
    "                 label = 'A', values = [1,4]),\n",
    "            dict(range = [1.5,5],\n",
    "                 tickvals = [1.5,3,4.5],\n",
    "                 label = 'B', values = [3,1.5]),\n",
    "            dict(range = [1,5],\n",
    "                 tickvals = [1,2,4,5],\n",
    "                 label = 'C', values = [2,4],\n",
    "                 ticktext = ['text 1', 'text 2', 'text 3', 'text 4']),\n",
    "            dict(range = [1,5],\n",
    "                 label = 'D', values = [4,2])\n",
    "        ])\n",
    "    )\n",
    "]\n",
    "\n",
    "py.iplot(data, filename = 'parcoord-dimensions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pandas.tools.plotting import parallel_coordinates\n",
    "parallel_coordinates (dataset, 'species')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maps\n",
    "* Allows us to plot points geographically\n",
    "* We can overlay information on a map, usually loaded as a collection of ‘tiles’. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "map_object = folium.Map(location=[-33.8, 151.2], zoom_start=2,\n",
    "tiles=\"Stamen toner\")\n",
    "marker = folium.features.Marker([-33.869824, 151.206423],popup=\"General Assembly!\")\n",
    "map_object.add_child(marker) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_airports = pd.read_csv('https://raw.githubusercontent.com/plotly/datasets/master/2011_february_us_airport_traffic.csv')\n",
    "df_airports.head()\n",
    "\n",
    "df_flight_paths = pd.read_csv('https://raw.githubusercontent.com/plotly/datasets/master/2011_february_aa_flight_paths.csv')\n",
    "df_flight_paths.head()\n",
    "\n",
    "airports = [ dict(\n",
    "        type = 'scattergeo',\n",
    "        locationmode = 'USA-states',\n",
    "        lon = df_airports['long'],\n",
    "        lat = df_airports['lat'],\n",
    "        hoverinfo = 'text',\n",
    "        text = df_airports['airport'],\n",
    "        mode = 'markers',\n",
    "        marker = dict(\n",
    "            size=2,\n",
    "            color='rgb(255, 0, 0)',\n",
    "            line = dict(\n",
    "                width=3,\n",
    "                color='rgba(68, 68, 68, 0)'\n",
    "            )\n",
    "        ))]\n",
    "\n",
    "flight_paths = []\n",
    "for i in range( len( df_flight_paths ) ):\n",
    "    flight_paths.append(\n",
    "        dict(\n",
    "            type = 'scattergeo',\n",
    "            locationmode = 'USA-states',\n",
    "            lon = [ df_flight_paths['start_lon'][i], df_flight_paths['end_lon'][i] ],\n",
    "            lat = [ df_flight_paths['start_lat'][i], df_flight_paths['end_lat'][i] ],\n",
    "            mode = 'lines',\n",
    "            line = dict(\n",
    "                width = 1,\n",
    "                color = 'red',\n",
    "            ),\n",
    "            opacity = float(df_flight_paths['cnt'][i])/float(df_flight_paths['cnt'].max()),\n",
    "        )\n",
    "    )\n",
    "\n",
    "layout = dict(\n",
    "        title = 'Feb. 2011 American Airline flight paths<br>(Hover for airport names)',\n",
    "        showlegend = False,\n",
    "        height = 800,\n",
    "        geo = dict(\n",
    "            scope='north america',\n",
    "            projection=dict( type='azimuthal equal area' ),\n",
    "            showland = True,\n",
    "            landcolor = 'rgb(243, 243, 243)',\n",
    "            countrycolor = 'rgb(204, 204, 204)',\n",
    "        ),\n",
    "    )\n",
    "\n",
    "fig = dict( data=flight_paths + airports, layout=layout )\n",
    "\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Charts to avoid\n",
    "* Stacked area maps\n",
    "* Word clouds\n",
    "* Pie charts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional readings\n",
    "* Edward Tuſte, The Visual Display of Quantitative Information\n",
    "* Leland Wilkinson, The Grammar of Graphics\n",
    "* Scott Murray, Interactive Data Visualisation for the Web (free online)\n",
    "* flowingdata.com\n",
    "* New York Times (Upshot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 2 Lesson 2\n",
    "## Linear Regression\n",
    "4 - 1 June 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised vs unsupervised learning\n",
    "* Supervised learning is where there is a target output - we want something to come out of our model.\n",
    " * Regression: If the target variable is numeric then we have a regression problem.\n",
    " * cf. Classification: If the target variable is a category (for example trying to predict a type of flower) the we have a classification problem - we are trying to classify what group that y belongs to.\n",
    " * Features: Data values that provide information to help guess the target, aka predictor or independent variables.\n",
    "* Unsupervised learning is where there is no output, but we want to observe the model of the data, e.g. for exploration. We want to find some underlying structure or patterns in the data but in this case we don’t have any labeled data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression \n",
    "* We want to model a linear relationship (think straight line) between our target variable y and our input variable x:\n",
    "y=Xβ+ϵ, where:\n",
    " * y = target variable\n",
    " * X = input variable\n",
    " * β = coefficients\n",
    " * ϵ = error term \n",
    "* It is the explanation of a continuous variable given a series of independent variables.\n",
    "* The simplest version is just a line of best fit: y = mx + b... It explains the relationship between x and y using the starting point b and the power in explanation m.\n",
    "* Linear regression is a simple approach to supervised learning. It assumes that the dependence of Y on X1, X2 ,… Xp is linear. True regression functions are never linear.\n",
    "* Linear relationship in the parameters, β, we can transform the actual values of the inputs if we want.\n",
    "* Variance of the error term, ϵ, is constant. This means there is no systematic pattern in the values of X and the variance of ϵ. The mean of ϵ = 0. ϵ has a normal distribution. If it does not, it could introduce bias. \n",
    "* There will be no perfect (or near perfect) co-linearity between any of the input variables. Otherwise the fitting procedure will break. \n",
    "\n",
    "\n",
    "#### How it works\n",
    "_Ordinary Least Squares_\n",
    "<img src=\"linear-regression-rss.png\" align=right width=300>\n",
    "* Creating a linear regression model is really about minimising the _Residual Sum of Squares_ or RSE. This is the Sum of the squared difference between our observed value and the value from the model. That is: <img src=\"RSS-equation.png\" width=400>\n",
    "* Wording the function as: <img src=\"linear-equation.png\">\n",
    "* Wording RSS as a function of e (error): <img src=\"rssaserror.png\">\n",
    "* Error is just the difference between yi and y as it is observed. So, expanding: <img src=\"rssaserror2.png\">\n",
    "* Resolving this equation, it means:\n",
    "<img src=\"OLS.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### R-Squared\n",
    "* R-squared is the central metric introduced for linear regression. The closer to 1 the R-squared, the better the fit. \n",
    "* R-squared measures explain variance, but it doesn't tell the magnitude or scale of error.\n",
    "\n",
    "<img src=\"R2-equations.png\" width=500 align=left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple linear regression\n",
    "* Multi-dimensions allows for complex models even with linear components.\n",
    "\n",
    "<img src=\"multiple-linear-regression2.png\" align=right width=200 >\n",
    "\n",
    "* The ideal scenario is when the predictors are uncorrelated:\n",
    " * Interpretations can be made such as “a unit change in Xj is associated with a βj change in Y , while all the other variables stay fixed”.\n",
    " * Correlations amongst predictors cause problems when Xj changes, everything else changes.\n",
    "\n",
    " <img src=\"multiple-linear-regression.png\" align=left width=400>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make sure you visualise your data and check the actual model fit !!! \n",
    "See [Anscombe's Quartet](https://en.wikipedia.org/wiki/Anscombe%27s_quartet) for why!\n",
    "<img src=\"anscombes-quartet.png\" width=400 align=right>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "house_data = pd.read_csv(\"../data/chicagohouseprices2.csv\", index_col=0)\n",
    "house_data.head(1)\n",
    "#replace missing data\n",
    "house_data.fillna(value='NA', inplace=True) \n",
    "#look at the correlations between the data\n",
    "pd.scatter_matrix(house_data, figsize=(15,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create the linear model\n",
    "lm1 = smf.ols(formula='Price ~ Bath + HouseSizeSqft', data=house_data).fit()\n",
    "#print the coefficients\n",
    "lm1.params\n",
    "#print the regression results\n",
    "lm1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lm2 = smf.ols(formula='Price ~ Bath + CrimeIndex + MinutesToLoop + MilesToLake + Age + LotSizeSqft + HouseSizeSqft + SchoolIndex + EstimatedPrice', data=house_data).fit()\n",
    "lm2.params\n",
    "lm2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 3 Lesson 1\n",
    "## Logistic Regression\n",
    "5 - 6 June 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression vs Classification\n",
    "* If the y variable is numeric then we have a regression problem - we are trying to predict a continuous number\n",
    "* If the y variable is a category (for example trying to predict a type of flower) the we have a classification problem - we are trying to classify what group that y belongs to.\n",
    "* We want to build a classifier that correctly identifies which class our target variable y belongs to given our input variable x. \n",
    "* Why not use the linear regression model? y=Xβ+ϵ\n",
    " * If we only have a binary response variable (0 or 1) it might make sense… BUT we can have our estimated value of y > 1 or y < 0 … which doesn’t make sense.\n",
    " * What of the case where we have more than one class? Linear regression cannot easily handle these cases.\n",
    " * We want a classification method that can handle these cases and give us results we can easily interpret. \n",
    "\n",
    "<img src=\"logit-function.png\" width=400 align=right>\n",
    "### p(1|x) = β0+β1\n",
    "* This is a good starting point but we still have the problem of p(Y) being outside the 0,1 range.\n",
    "* We need to model p(Y=1|X) using a function that gives outputs between 0 and 1. Basically we want something that looks like the following:\n",
    "<img src=\"logistic-regression.png\" width=300 align=right>\n",
    "\n",
    "\n",
    "\n",
    "* We can see that it this function is linear in X\n",
    " * p | (p - 1) is called the ‘odds’ and can be any value from 0 to ∞\n",
    " * log ( p | (p - 1) ) is called the ‘log-odds’ or ‘logit’"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Score\n",
    "* This is simply the fraction of correct predictions from the model. So it is the number of correct predictions divided by the number of observations in our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"confusion-matrix.png\" align=right>\n",
    "### Confusion Matrix\n",
    "* A confusion matrix shows us what the predicted class was against what the actual class was. \n",
    "* The true class makes up the rows or the vertical axes and the predicted class makes up the columns or the horizontal axis.\n",
    "* Any entries in the diagonal of the matrix are those that are correctly classified. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"roc-curve.png\" align=right>\n",
    "### Receiver Operating Characteristic\n",
    "\n",
    "* The Receiver Operating Characteristic or ROC curve shows the performance of a binary classifier system as its discrimination threshold is varied. \n",
    "* It is created by plotting the fraction of true positives out of the positives (TPR = true positive rate) vs. the fraction of false positives out of the negatives (FPR = false positive rate), at various threshold settings.\n",
    "* By computing the Area Under the Curve of the ROC curve we get a single number summary of accuracy.\n",
    "* The closer that number is to 1 the more accurate our model is. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#read the data\n",
    "titanic = pd.read_csv('../data/titanic.csv', index_col='PassengerId')\n",
    "#define the columns to read (x), and the target (y)\n",
    "feature_cols = ['Pclass', 'Parch']\n",
    "X = titanic[feature_cols]\n",
    "y = titanic.Survived\n",
    "#split the data\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "#fit the logreg model and print the coefficients\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "zip(feature_cols, logreg.coef_[0])\n",
    "print(logreg.fit(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#make predictions on testing set and test accuracy\n",
    "y_pred_class = logreg.predict(X_test)\n",
    "from sklearn import metrics\n",
    "print(metrics.accuracy_score(y_test, y_pred_class))\n",
    "#print the confusion matrix\n",
    "from sklearn import metrics\n",
    "prds = logreg.predict(X)\n",
    "print(metrics.confusion_matrix(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#generate the ROC curve\n",
    "import matplotlib.pyplot as plt\n",
    "# Generate the prediction values for each of the test observations using predict_proba() function rather than just predict\n",
    "preds = logreg.predict_proba(X_test)[:,1]\n",
    "# Store the false positive rate(fpr), true positive rate (tpr) in vectors for use in the graph\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test, preds)\n",
    "# Store the Area Under the Curve (AUC) so we can annotate our graph with this metric\n",
    "roc_auc = metrics.auc(fpr,tpr)\n",
    "# Plot the ROC Curve\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "# What's happening here is we are changing the cutoff value from 0 to 1.\n",
    "# When we have a cutoff of zero this means that we have no positive predictions so both fpr and tpr are both 0\n",
    "# Our aim when modelling is to maximise the area under the curve, the closer to one the better the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#split data into binaries where not binary\n",
    "titanic_with_dummies = pd.get_dummies(data=titanic, columns = ['Sex', 'Embarked', 'Pclass'], prefix = ['Sex', 'Embarked', 'Pclass'] )\n",
    "titanic_with_dummies['Age'] = titanic_with_dummies[['Age',\"Parch\",\"Sex_male\",'Pclass_1', 'Pclass_2']].groupby([\"Parch\",\"Sex_male\",'Pclass_1', 'Pclass_2'])['Age'].transform(lambda x: x.fillna(x.mean()))\n",
    "feature_cols = ['Parch', 'Sex_male', 'Sex_female', 'Pclass_1', 'Pclass_2', 'Pclass_3', 'Fare', 'Age']\n",
    "X = titanic_with_dummies[feature_cols]\n",
    "y = titanic_with_dummies.Survived\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "logreg.fit(X_train, y_train)\n",
    "zip(feature_cols, logreg.coef_[0])\n",
    "y_pred_class = logreg.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred_class))\n",
    "preds = logreg.predict_proba(X_test)[:,1]\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test, preds)\n",
    "roc_auc = metrics.auc(fpr,tpr)\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "KNN_model = KNeighborsClassifier(5)\n",
    "KNN_model.fit(X_train, y_train)\n",
    "y_pred_class = KNN_model.predict(X_test)\n",
    "# Print the new accuracy rate\n",
    "print(metrics.accuracy_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "NB_model = GaussianNB()\n",
    "NB_model.fit(X_train, y_train)\n",
    "y_pred_class = NB_model.predict(X_test)\n",
    "# Print the new accuracy rate\n",
    "print(metrics.accuracy_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional readings\n",
    "* Logistic Regression applied to loan applications https://github.com/nborwankar/LearnDataScience\n",
    "* Odds Ratio in Logistic Regression http://www.ats.ucla.edu/stat/mult_pkg/faq/general/odds_ratio.htm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 3 Lesson 2\n",
    "## Model Evaluation\n",
    "6 - 8 June 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is model evaluation?\n",
    "* Model evaluation is how we test whether our model works on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why do we do model evaluation?\n",
    "* A model bsed on training data has a set error - the training error - for how well the model fits training data. This is a good way to see how the model fits _that_ data, but not all data. \n",
    "* We want to test other data on the model for a better measure of accuracy.\n",
    "* Without model evaluation, you run the risk of over-fitting the training data, which will have high accuracy for the training data but low accuracy for everything else.\n",
    "* This is what we call **bias**: if your model has too much bias, it will be accurate for the training data, but will be too closely fit to the data and so won't have the same accuracy for new data.\n",
    "* **Variance** is the opposite: if your model has too much variance, it won't be accurate for the training data, because it's not fit well enough.\n",
    "<img src=\"bias-and-variance.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### High Variance, Low Bias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_table('http://people.sc.fsu.edu/~jburkardt/datasets/regression/x01.txt', sep='\\s+', skiprows=33, names=['id','brain','body'], index_col='id')\n",
    "np.random.seed(12345)\n",
    "df = df[df.body < 200]\n",
    "df['sample'] = np.random.randint(1, 3, len(df))\n",
    "\n",
    "sns.lmplot(x='body', y='brain', data=df, ci=None, hue='sample')\n",
    "sns.plt.xlim(-10, 200)\n",
    "sns.plt.ylim(-10, 250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Low Variance, High Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.lmplot(x='body', y='brain', data=df, ci=None, hue='sample', order=8)\n",
    "sns.plt.xlim(-10, 200)\n",
    "sns.plt.ylim(-10, 250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Good balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.lmplot(x='body', y='brain', data=df, ci=None, hue='sample', order=2)\n",
    "sns.plt.xlim(-10, 200)\n",
    "sns.plt.ylim(-10, 250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we do model evaluation?\n",
    "* We do model evaluation by separating out training data and test data from our dataset, and using the training data to build the model. We then test its accuracy on test data. This is the out-of-sample error (OOS).\n",
    "* If we had a different training/test split, the OOS error would be different.\n",
    "* One way of improving this will be to use multiple splits for test and training data. Train the model with different splits of data, and test with the rest from that split.\n",
    "* This is called **cross-validation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#We'll be using logistic regression with Titanic dataset:\n",
    "import pandas as pd\n",
    "titanic = pd.read_csv('../data/titanic.csv', index_col='PassengerId')\n",
    "#Imputing values for Age:\n",
    "from sklearn.preprocessing import Imputer\n",
    "imp = Imputer(strategy='mean', axis=1)\n",
    "titanic['Age'] = imp.fit_transform(titanic.Age.values.reshape(1,-1)).T\n",
    "#Logreg on Age:\n",
    "feature_cols = ['Pclass', 'Parch','Age']\n",
    "X = titanic[feature_cols]\n",
    "y = titanic.Survived\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(C=1e9)\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred_class = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-validation\n",
    "* Steps for K-fold cross-validation:\n",
    " 1. Randomly split the dataset into K equal partitions.\n",
    " 2. Use partition 1 as test set & union of other partitions as training set.\n",
    " 3. Calculate test set error.\n",
    " 4. Repeat steps 2-3 using a different partition as the test set at each iteration.\n",
    " 5. Take the average test set error as the estimate of OOS accuracy.\n",
    "* Features of K-fold cross-validation:\n",
    " 1. More accurate estimate of OOS prediction error.\n",
    " 2. More efficient use of data than single train/test split.\n",
    "   * Each record in our dataset is used for both training and testing.\n",
    " 3. Presents tradeoff between efficiency an computational expense.\n",
    "   * 10-fold CV is 10x more expensive than a single train/test split\n",
    " 4. Can be used for parameter tuning and model selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titanicwd = pd.get_dummies(data=titanic, columns = ['Sex', 'Embarked', 'Pclass'], prefix = ['Sex', 'Embarked', 'Pclass'] )\n",
    "# Fill any null values of age by taking average by gender, class and parch\n",
    "titanicwd['Age']=titanicwd[[\"Age\",\"Parch\",\"Sex_male\",\"Pclass_1\",\"Pclass_2\"]].groupby([\"Parch\",\"Sex_male\",'Pclass_1', 'Pclass_2'])['Age'].transform(lambda x: x.fillna(x.mean()))\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "feature_cols = ['Sex_male', 'Sex_female','Embarked_C','Embarked_Q','Embarked_S','Pclass_1', 'Pclass_2', 'Pclass_3','Parch', 'Age','SibSp']\n",
    "X = titanicwd[feature_cols]\n",
    "y = titanicwd.Survived\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "logreg.fit(X_train, y_train)\n",
    "zip(feature_cols, logreg.coef_[0])\n",
    "y_pred_class = logreg.predict(X_test)\n",
    "\n",
    "print(metrics.accuracy_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scores = cross_val_score(LogisticRegression(), X, y, scoring='accuracy', cv=10)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import cross_validation\n",
    "from sklearn import linear_model\n",
    "mse_values = []\n",
    "scores = []\n",
    "n = 0\n",
    "\n",
    "feature_cols = ['Sex_male', 'Sex_female','Embarked_C','Embarked_Q','Embarked_S','Pclass_1', 'Pclass_2', 'Pclass_3',]\n",
    "modeldata = titanicwd[feature_cols]\n",
    "y = titanicwd.Survived \n",
    "\n",
    "kf = cross_validation.KFold(len(modeldata), n_folds=10, shuffle=True)\n",
    "\n",
    "print(\"~~~~ CROSS VALIDATION each fold ~~~~\")\n",
    "for train_index, test_index in kf:\n",
    "    lm = linear_model.LinearRegression().fit(modeldata.iloc[train_index], y.iloc[train_index])\n",
    "    mse_values.append(metrics.mean_squared_error(y.iloc[test_index], lm.predict(modeldata.iloc[test_index])))\n",
    "    scores.append(lm.score(modeldata, y))\n",
    "    n+=1\n",
    "    print ('Model', n)\n",
    "    print ('MSE:', mse_values[n-1])\n",
    "    print ('R2:', scores[n-1])\n",
    "\n",
    "\n",
    "print (\"~~~~ SUMMARY OF CROSS VALIDATION ~~~~\")\n",
    "print ('Mean of MSE for all folds:', np.mean(mse_values))\n",
    "print ('Mean of R2 for all folds:', np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Null accuracy\n",
    "* **Null accuracy** is the accuracy that could be achieved by always predicting the **most frequent class**. It is a baseline against which you may want to measure your classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (y_test.mean())\n",
    "print (1 - y_test.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "dumb = DummyClassifier(strategy='most_frequent')\n",
    "dumb.fit(X_train, y_train)\n",
    "y_dumb_class = dumb.predict(X_test)\n",
    "print (metrics.accuracy_score(y_test, y_dumb_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion matrix\n",
    "* Confusion Matrix is another way to evaluate a model. It is a table to describe the performance of a classifier.\n",
    "* **Sensitivity | Recall** = (number of true positives) | (true positives + false negatives - as in, number of all real positives)\n",
    "* **Specificity** = (number of true negatives) | (true negatives + false positives - as in number of all real negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "metrics.confusion_matrix(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load confusion_matrix_nice.py\n",
    "#from confusion_matrix_nice import plot_confusion_matrix\n",
    "%run confusion_matrix_nice\n",
    "cnf_mat = metrics.confusion_matrix(y_test, y_pred_class, labels = titanic.Survived.unique())\n",
    "class_labels = titanic.Survived.unique()\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_mat, class_labels,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sensitivity = TP / TP + FN\n",
    "#specificity = TN / TN + FP\n",
    "sensitivity=43/(43+52)\n",
    "specificity = 107/(107+21)\n",
    "print(sensitivity,specificity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred_prob = logreg.predict_proba(X_test)[:, 1]\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(y_pred_prob)\n",
    "plt.xlabel('Predicted probability of survival')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "y_pred_class = np.where(y_pred_prob > 0.25, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (metrics.confusion_matrix(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sensitivity = 68 / (27 + 68)\n",
    "specificity = 57/ (57 + 71)\n",
    "print(sensitivity,specificity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision + recall\n",
    "* **Precision** = (number of true positives) | (true positives + false positives - as in number of all predicted positives)\n",
    "* **Sensitivity | Recall** = (number of true positives) | (true positives + false negatives - as in, number of all real positives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Accuracy score returns the percent of true positives and true negatives samples (in decimal format with 1 being perfect)\n",
    "print('Accuracy score:',metrics.accuracy_score(y_test, y_pred_class))\n",
    "#Precision score returns the correctly identified \n",
    "# = # true positives / true positives + false positives:\n",
    "print('Precision score:',metrics.precision_score(y_test, y_pred_class))\n",
    "#Recall score returns the # of  positives correctly predicted \n",
    "# = # true positives / true positives + false negatives:\n",
    "print('Recall score:',metrics.recall_score(y_test,y_pred_class))\n",
    "# The F1 score can be interpreted as a weighted average of the precision and recall.\n",
    "# An F1 score reaches its best value at 1 and worst score at 0. \n",
    "# The relative contribution of precision and recall to the F1 score are equal. \n",
    "# F1 = 2 * (precision * recall) / (precision + recall)\n",
    "print('F1 score:',metrics.f1_score(y_test,y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional readings\n",
    "* Caltech's Learning From Data course visualising bias and variance (15 mins) http://work.caltech.edu/library/081.html\n",
    "* Rahul Patwari has a great video on ROC Curves (12 minutes)  https://www.youtube.com/watch?v=21Igj5Pr6u4\n",
    "* Have a look at scikit-learn’s documentation on model evaluation http://scikit-learn.org/stable/modules/model_evaluation.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 4 Lesson 1\n",
    "## Regularisation\n",
    "7 - 13 June 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset selection\n",
    "* We could fit a separate linear regression model for every combination of our features. But what happens when we have a large number of features? Computation time becomes a factor and we also need to consider that as we include more features we are increasing the chance we include a variable that doesn’t add any predictive power for future data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "crime = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/communities/communities.data', header=None, na_values=['?'])\n",
    "crime = crime.iloc[:, 5:]\n",
    "crime.dropna(inplace=True)\n",
    "crime.head()\n",
    "X = crime.iloc[:, :-1]\n",
    "y = crime.iloc[:, -1]\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lm = LinearRegression()\n",
    "lm.fit(X_train, y_train)\n",
    "lm.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularisation\n",
    "* Regularisation works by using a tuning parameter lambda or alpha that imposes a penalty on the size of coefficients.\n",
    "* Instead of minimizing the \"loss function\" (mean squared error), it minimizes the \"loss plus penalty\".\n",
    "* A tiny alpha imposes no penalty on the coefficient size, and is equivalent to a normal linear model.\n",
    "* Increasing the alpha penalizes the coefficients and shrinks them toward zero.\n",
    "\n",
    "* Recall from Week 2 that the ordinary least squares procedure estimates coefficients that minimise:\n",
    "<img src=\"RSS-equation-again.png\">\n",
    "* Regularization (or Shrinkage) is a way to constrain the estimates of beta to be close or equal to zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "preds = lm.predict(X_test)\n",
    "print('RMSE (no regularization) =', np.sqrt(metrics.mean_squared_error(y_test, preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge Regression regularisation\n",
    "* Ridge Regression is similar to least squares, except we include a penalty term, the λ term is a tuning parameter. \n",
    "* When it is zero we get least squares, as it increases the term, (the shrinkage penalty) has more of an impact and the coefficients will **approach** zero.\n",
    "<img src=\"ridge-regression.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "rreg = Ridge(alpha=0.1, normalize=True)\n",
    "rreg.fit(X_train, y_train)\n",
    "rreg.coef_\n",
    "preds = rreg.predict(X_test)\n",
    "print('RMSE (Ridge reg.) =', np.sqrt(metrics.mean_squared_error(y_test, preds)))\n",
    "# Is this model better? Why?\n",
    "# Yes, this model is a better result. It is removing those coefficients / columns that don't play a part in predicting the value of X.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use RidgeCV to select best alpha\n",
    "from sklearn.linear_model import RidgeCV\n",
    "alpha_range = 10.**np.arange(-2, 3)\n",
    "rregcv = RidgeCV(normalize=True, scoring='neg_mean_squared_error', alphas=alpha_range)\n",
    "rregcv.fit(X_train, y_train)\n",
    "# Print the optimal value of Alpha for Ridge Regression\n",
    "print('Optimal Alpha Value: ', rregcv.alpha_)\n",
    "# Print the RMSE for the ridge regression model\n",
    "preds = rregcv.predict(X_test)\n",
    "print ('RMSE (Ridge CV reg.) =', np.sqrt(metrics.mean_squared_error(y_test, preds)))\n",
    "# What is the range of alpha values we are searching over?\n",
    "# We are searching from 0.01 (10^-2) and 1000 (10^3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lasso Regression regularisation\n",
    "Lasso Regression is similar to Ridge Regression, except we have the absolute value of beta in our penalty term, the λ term is a tuning parameter. \n",
    "When it is zero we get least squares, as it increases the term, (the shrinkage penalty) has more of an impact and the coefficients will **equal** zero.\n",
    "<img src=\"lasso-regression.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"comparelassoridge.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Lasso regularization is useful if we believe many features are irrelevant, since a feature with a zero coefficient is essentially removed from the model. Thus, it is a useful technique for feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "########## Lasso Regression Model ##########\n",
    "# lasso (alpha must be positive, larger means more regularization)\n",
    "from sklearn.linear_model import Lasso\n",
    "las = Lasso(alpha=0.01, normalize=True)\n",
    "las.fit(X_train, y_train)\n",
    "las.coef_\n",
    "preds = las.predict(X_test)\n",
    "print('RMSE (Lasso reg.) =', np.sqrt(metrics.mean_squared_error(y_test, preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use LassoCV to select best alpha (tries 100 alphas by default)\n",
    "from sklearn.linear_model import LassoCV\n",
    "alpha_range = 10.**np.arange(-10, 10)\n",
    "lascv = LassoCV(normalize=True, alphas=alpha_range)\n",
    "lascv.fit(X_train, y_train)\n",
    "print('Optimal Alpha Value: ',lascv.alpha_)\n",
    "lascv.coef_\n",
    "preds = lascv.predict(X_test)\n",
    "print('RMSE (Lasso CV reg.) =', np.sqrt(metrics.mean_squared_error(y_test, preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "enet = ElasticNet(alpha=0.1, l1_ratio=0.7)\n",
    "enet.fit(X_train, y_train)\n",
    "enet.coef_\n",
    "preds=enet.predict(X_test)\n",
    "print('RMSE (ENET reg.)', np.sqrt(metrics.mean_squared_error(y_test, preds)))\n",
    "print('-----')\n",
    "\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "enetCV = ElasticNetCV(normalize=True, alphas=alpha_range)\n",
    "enetCV.fit(X_train, y_train)\n",
    "print('Optimal Alpha Value: ',enetCV.alpha_)\n",
    "enetCV.coef_\n",
    "preds = enetCV.predict(X_test)\n",
    "print('RMSE (ENET CV reg.)', np.sqrt(metrics.mean_squared_error(y_test, preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 4 Lesson 2\n",
    "## Clustering\n",
    "8 - 15 June 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is clustering?\n",
    "* Recall unsupervised learning is when we are trying to find interesting patterns or groups in our data. We don’t have a variable we are trying to predict (a Y value).\n",
    "* Clustering aims to discover subgroups in our data where the points are similar to each other. So we have a collection of groups and all points belonging to the same group are similar. Points in different groups are different to each other.\n",
    "* We have to decide what variables we will construct the groups on. What makes them different (or similar)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why do we cluster data?\n",
    "* To enhance our understanding of a dataset by dividing the data into groups.\n",
    "* Clustering provides a layer of abstraction from individual data points.\n",
    "* The goal is to extract and enhance the natural structure of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we cluster data?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-means clustering\n",
    "1. Choose k initial centroids (note that k is an input):\n",
    " * randomly (but may yield divergent behavior)\n",
    " * perform alternative clustering task, use resulting centroids as initial k-means centroids\n",
    " * start with global centroid, choose point at max distance, repeat (but might select outlier)\n",
    "2. For each point:\n",
    " * find distance to each centroid\n",
    " * assign point to nearest centroid:\n",
    "  * The similarity criterion is determined by the measure we choose.\n",
    "  * In the case of k-means clustering, the similarity metric is the Euclidian distance:\n",
    "  <img src=\"euclidian-distance.png\">\n",
    "3. Recalculate centroid positions.\n",
    "4. Repeat steps 2-3 until stopping criteria met.\n",
    " * We iterate until some stopping criteria are met; in general, suitable convergence is achieved in a small number of steps. \n",
    " * Stopping criteria can be based on the centroids (eg, if positions change by no more than ε) or on the points (eg, if no more than x% change clusters between iterations).\n",
    " \n",
    "See this great visualisation: http://shabal.in/visuals/kmeans/6.html\n",
    "\n",
    "* Strengths: K-means is a popular algorithm because of its computational efficiency and simple and intuitive nature.\n",
    "* Weaknesses: However, K-means is highly scale dependent, and is not suitable for data with widely varying shapes and densities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "d = iris.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "est = KMeans(n_clusters=3, init='random')\n",
    "est.fit(d)\n",
    "y_kmeans = est.predict(d)\n",
    "#plot it up with centres\n",
    "colors = np.array(['#FF0054','#FBD039','#23C2BC'])\n",
    "plt.figure()\n",
    "plt.scatter(d[:, 2], d[:, 0], c=colors[y_kmeans], s=50)\n",
    "plt.xlabel(iris.feature_names[2])\n",
    "plt.ylabel(iris.feature_names[0])\n",
    "plt.scatter(centers[:, 2], centers[:, 0], c='k', linewidths=3,\n",
    "            marker='+', s=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster validation (the Silhouette Coefficient)\n",
    "* Cohesion measures clustering effectiveness within a cluster.\n",
    "<img src=\"cohesion.png\">\n",
    "* Separation measures clustering effectiveness between clusters.\n",
    "<img src=\"separation.png\">\n",
    "* A useful measure that combines the ideas of cohesion and separation is the silhouette coefficient. For point xi, this is given by: \n",
    "<img src=\"silhouette-coefficient.png\">\n",
    "* such that: \n",
    " * ai = average in-cluster distance to xi\n",
    " * bij = average between-cluster distance to xi\n",
    " * bi = minj(bij).\n",
    "* The silhouette coefficient can take values between -1 and 1.\n",
    "* In general, we want separation to be high and cohesion to be low. \n",
    "* This corresponds to a value of SC close to +1. A negative silhouette coefficient means the cluster radius is larger than the space between clusters, and thus clusters overlap\n",
    "* One useful application of cluster validation and the silhouette coefficient is to determine the best number of clusters for your dataset.  \n",
    "* Ultimately, cluster validation and clustering in general are suggestive techniques that rely on human interpretation to be meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "'''\n",
    "K = 2\n",
    "'''\n",
    "\n",
    "# Generate and Plot Dummy Data for k = 2\n",
    "centres = [[2, 0.75], [0, 0]]\n",
    "X0, labels0_true = make_blobs(n_samples=300, centers=centres[0], cluster_std=[[0.2,0.2]])\n",
    "X1, labels1_true = make_blobs(n_samples=300, centers=centres[1], cluster_std=[[0.2,0.2]])\n",
    "X = np.concatenate((X0,X1))\n",
    "labels_true = np.concatenate((labels0_true,labels1_true+1))\n",
    "colors = np.array(['#FF0054','#FBD039'])\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.suptitle('Cluster Validation Evaluation', fontsize=15)\n",
    "plt.subplot(331)\n",
    "plt.text(-0.5, 1.5, 'k=2', fontsize=14)\n",
    "for k, col in zip(range(2), colors):\n",
    "    my_members = labels_true == k\n",
    "    cluster_center = centres[k]\n",
    "    plt.scatter(X[my_members, 0], X[my_members, 1], c=col, marker='o',s=20) \n",
    "    plt.scatter(cluster_center[0], cluster_center[1], c=col, marker='o', s=200)\n",
    "plt.axis('equal')\n",
    "plt.title('Data with truth labels')\n",
    "plt.ylabel('y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k_rng = range(1,15)\n",
    "est = [KMeans(n_clusters = k).fit(X) for k in k_rng]\n",
    "silhouette_score = [metrics.silhouette_score(X, e.labels_, metric='euclidean') for e in est[1:]]\n",
    "plt.plot(k_rng[1:], silhouette_score, 'b*-')\n",
    "plt.xlim([1,15])\n",
    "plt.grid(True)\n",
    "plt.title('Silhouette Coefficient')\n",
    "plt.plot(2,silhouette_score[0], 'o', markersize=12, markeredgewidth=1.5,\n",
    "         markerfacecolor='None', markeredgecolor='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate the within sum of squared errors for different values of K, highlighting k = 2\n",
    "within_sum_squares = [e.inertia_ for e in est]\n",
    "plt.plot(k_rng, within_sum_squares, 'b*-')\n",
    "plt.xlim([1,15])\n",
    "plt.grid(True)\n",
    "plt.title('Within Sum of Squared Errors')\n",
    "plt.plot(2,within_sum_squares[1], 'ro', markersize=12, markeredgewidth=1.5,\n",
    "         markerfacecolor='None', markeredgecolor='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limitations of K-Means\n",
    "\n",
    "Adapted from Bart Baddely's 2014 PyData Presentation: http://nbviewer.ipython.org/github/BartBaddeley/PyDataTalk-2014/blob/master/PyDataTalk.ipynb\n",
    "\n",
    "Agenda: \n",
    "1. K-means might not work when dimensions have different scales\n",
    "2. K-means might not work for non-spherical shapes\n",
    "3. K-means might not work for clusters of different sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "from sklearn.datasets.samples_generator import make_blobs, make_moons\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#1) DIMENSIONS WITH DIFFERENT SCALES\n",
    "\n",
    "# Generate data with differing variances\n",
    "np.random.seed(0)\n",
    "\n",
    "centres = [[1, 0.75], [1, -0.75], [0, 0]]\n",
    "\n",
    "X0, labels0_true = make_blobs(n_samples=300, centers=centres[0], cluster_std=[[0.6,0.1]])\n",
    "X1, labels1_true = make_blobs(n_samples=300, centers=centres[1], cluster_std=[[0.6,0.1]])\n",
    "X2, labels2_true = make_blobs(n_samples=300, centers=centres[2], cluster_std=[[0.6,0.1]])\n",
    "X = np.concatenate((X0,X1,X2))\n",
    "labels_true = np.concatenate((labels0_true,labels1_true+1,labels2_true+2))\n",
    "\n",
    "colors = np.array(['#FF0054','#FBD039','#23C2BC'])\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.suptitle('Dimensions with Different Scales', fontsize=15)\n",
    "for k, col in zip(range(3), colors):\n",
    "    my_members = labels_true == k\n",
    "    cluster_center = centres[k]\n",
    "    plt.scatter(X[my_members, 0], X[my_members, 1], c=col, marker='o',s=20) \n",
    "    plt.scatter(cluster_center[0], cluster_center[1], c=col, marker='o', s=200)\n",
    "plt.axis('equal')\n",
    "plt.title('Original data')\n",
    "\n",
    "# Compute clustering with 3 Clusters\n",
    "k_means_3 = KMeans(init='k-means++', n_clusters=3, n_init=10)\n",
    "k_means_3.fit(X)\n",
    "k_means_3_labels = k_means_3.labels_\n",
    "k_means_3_cluster_centres = k_means_3.cluster_centers_\n",
    "\n",
    "# Plot result\n",
    "distance = euclidean_distances(k_means_3_cluster_centres,\n",
    "                               centres,\n",
    "                               squared=True)\n",
    "order = distance.argmin(axis=0)\n",
    "for k, col in zip(range(3), colors):              \n",
    "    my_members = k_means_3_labels == order[k]\n",
    "    plt.scatter(X[my_members, 0], X[my_members, 1],c=col, marker='o', s=20)           \n",
    "    cluster_center = k_means_3_cluster_centres[order[k]]\n",
    "    plt.scatter(cluster_center[0], cluster_center[1], marker = 'o', c=col, s=200, alpha=0.8)            \n",
    "plt.axis('equal')\n",
    "plt.title('KMeans 3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#2: NON-SPHERICAL SHAPES\n",
    "\n",
    "[X, true_labels] = make_moons(n_samples=1000, noise=.05)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.suptitle('Non-Spherical Shapes', fontsize=15)\n",
    "for k, col in zip(range(2), colors):\n",
    "    my_members = true_labels == k\n",
    "    plt.scatter(X[my_members, 0], X[my_members, 1], c=col, marker='o', s=20)\n",
    "\n",
    "plt.axis('equal')\n",
    "plt.title('Original Data') \n",
    "    \n",
    "# Compute clustering with 2 Clusters\n",
    "k_means_2 = KMeans(init='k-means++', n_clusters=2, n_init=10)\n",
    "k_means_2.fit(X)\n",
    "k_means_2_labels = k_means_2.labels_\n",
    "k_means_2_cluster_centers = k_means_2.cluster_centers_\n",
    "\n",
    "for k, col in zip(range(2), colors):           \n",
    "    my_members = k_means_2_labels == k\n",
    "    plt.scatter(X[my_members, 0], X[my_members, 1],c=col, marker='o', s=20)     \n",
    "    cluster_center = k_means_2_cluster_centers[k]\n",
    "    plt.scatter(cluster_center[0], cluster_center[1], marker = 'o', c=col, s=200, alpha=0.8) \n",
    "plt.axis('equal')\n",
    "plt.title('KMeans 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#3: CLUSTERS OF DIFFERENT SIZES\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "centres = [[-1, 0], [1, 0], [3, 0]]\n",
    "\n",
    "X0, labels0_true = make_blobs(n_samples=100, centers=centres[0], cluster_std=[[0.2,0.2]])\n",
    "X1, labels1_true = make_blobs(n_samples=400, centers=centres[1], cluster_std=[[0.6,0.6]])\n",
    "X2, labels2_true = make_blobs(n_samples=100, centers=centres[2], cluster_std=[[0.2,0.2]])\n",
    "X = np.concatenate((X0,X1,X2))\n",
    "labels_true = np.concatenate((labels0_true,labels1_true+1,labels2_true+2))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.suptitle('Clusters of Different Sizes', fontsize=15)\n",
    "for k, col in zip(range(3), colors):\n",
    "    my_members = labels_true == k\n",
    "    cluster_center = centres[k]\n",
    "    plt.scatter(X[my_members, 0], X[my_members, 1], c=col, marker='o',s=20) \n",
    "    plt.scatter(cluster_center[0], cluster_center[1], c=col, marker='o', s=200)\n",
    "plt.axis('equal')\n",
    "plt.title('Original data')\n",
    "\n",
    "# Compute clustering with 3 Clusters\n",
    "k_means_3 = KMeans(init='k-means++', n_clusters=3, n_init=10)\n",
    "k_means_3.fit(X)\n",
    "k_means_3_labels = k_means_3.labels_\n",
    "k_means_3_cluster_centres = k_means_3.cluster_centers_\n",
    "\n",
    "# Plot result\n",
    "distance = euclidean_distances(k_means_3_cluster_centres,\n",
    "                               centres,\n",
    "                               squared=True)\n",
    "order = distance.argmin(axis=0)\n",
    "for k, col in zip(range(3), colors):              \n",
    "    my_members = k_means_3_labels == order[k]\n",
    "    plt.scatter(X[my_members, 0], X[my_members, 1],c=col, marker='o', s=20)           \n",
    "    cluster_center = k_means_3_cluster_centres[order[k]]\n",
    "    plt.scatter(cluster_center[0], cluster_center[1], marker = 'o', c=col, s=200, alpha=0.8)            \n",
    "plt.axis('equal')\n",
    "plt.title('KMeans 3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DBSCAN clustering\n",
    "\n",
    "* Criteria:\n",
    " * ε (or Epsilon) is the radius \n",
    " * minPoints (number of points within the ε-Neighborhood required for classification)\n",
    "* DBSCAN iterates through every point\n",
    "* Core object is a point meeting the criteria.\n",
    "* Outlier is outside the radius.\n",
    "* Pros\n",
    " * Recovers more complex cluster shapes\n",
    " * Finds the number of clusters\n",
    " * Automatically find outliers\n",
    "* Cons\n",
    " * Requires a distance function\n",
    " * Not as scalable as K-means\n",
    " * Calculating connected components can be difficult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.datasets.samples_generator import make_blobs, make_moons\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1) DIMENSIONS WITH DIFFERENT SCALES\n",
    "# Generate data with differing variances\n",
    "np.random.seed(0)\n",
    "\n",
    "centres = [[1, 0.75], [1, -0.75], [0, 0]]\n",
    "\n",
    "X0, labels0_true = make_blobs(n_samples=300, centers=centres[0], cluster_std=[[0.6,0.1]])\n",
    "X1, labels1_true = make_blobs(n_samples=300, centers=centres[1], cluster_std=[[0.6,0.1]])\n",
    "X2, labels2_true = make_blobs(n_samples=300, centers=centres[2], cluster_std=[[0.6,0.1]])\n",
    "X = np.concatenate((X0,X1,X2))\n",
    "labels_true = np.concatenate((labels0_true,labels1_true+1,labels2_true+2))\n",
    "\n",
    "colors = np.array(['#FF0054','#FBD039','#23C2BC'])\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.suptitle('Dimensions with Different Scales', fontsize=15)\n",
    "for k, col in zip(range(3), colors):\n",
    "    my_members = labels_true == k\n",
    "    cluster_center = centres[k]\n",
    "    plt.scatter(X[my_members, 0], X[my_members, 1], c=col, marker='o',s=20) \n",
    "    plt.scatter(cluster_center[0], cluster_center[1], c=col, marker='o', s=200)\n",
    "plt.axis('equal')\n",
    "plt.title('Original data')\n",
    "\n",
    "# Compute clustering with 3 Clusters\n",
    "db = DBSCAN(eps=0.2, min_samples=5)\n",
    "db.fit(X)\n",
    "labels = db.labels_\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "\n",
    "db_csi = db.core_sample_indices_\n",
    "\n",
    "# Plot result\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "\n",
    "print('Estimated number of clusters: %d' % n_clusters_)\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels_true, labels))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels_true, labels))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels_true, labels))\n",
    "print(\"Adjusted Rand Index: %0.3f\"\n",
    "      % metrics.adjusted_rand_score(labels_true, labels))\n",
    "print(\"Adjusted Mutual Information: %0.3f\"\n",
    "      % metrics.adjusted_mutual_info_score(labels_true, labels))\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(X, labels))\n",
    "\n",
    "unique_labels = set(labels)\n",
    "colors = [plt.cm.Spectral(each)\n",
    "          for each in np.linspace(0, 1, len(unique_labels))]\n",
    "for k, col in zip(unique_labels, colors):\n",
    "    class_member_mask = (labels == k)\n",
    "    xy = X[class_member_mask & core_samples_mask]\n",
    "    plt.plot(xy[:, 0], xy[:, 1], 'o',markersize=5)\n",
    "    xy = X[class_member_mask & ~core_samples_mask]\n",
    "    plt.plot(xy[:, 0], xy[:, 1], 'o', markersize=6)\n",
    "plt.title('Estimated number of clusters: %d' % n_clusters_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#2: NON-SPHERICAL SHAPES\n",
    "\n",
    "[X, true_labels] = make_moons(n_samples=1000, noise=.05)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.suptitle('Non-Spherical Shapes', fontsize=15)\n",
    "for k, col in zip(range(2), colors):\n",
    "    my_members = true_labels == k\n",
    "    plt.scatter(X[my_members, 0], X[my_members, 1], c=col, marker='o', s=20)\n",
    "\n",
    "plt.axis('equal')\n",
    "plt.title('Original Data') \n",
    "    \n",
    "db = DBSCAN(eps=0.2, min_samples=5)\n",
    "db.fit(X)\n",
    "labels = db.labels_\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "\n",
    "# Plot result\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "\n",
    "print('Estimated number of clusters: %d' % n_clusters_)\n",
    "#print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels_true, labels))\n",
    "#print(\"Completeness: %0.3f\" % metrics.completeness_score(labels_true, labels))\n",
    "#print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels_true, labels))\n",
    "#print(\"Adjusted Rand Index: %0.3f\"\n",
    "     # % metrics.adjusted_rand_score(labels_true, labels))\n",
    "#print(\"Adjusted Mutual Information: %0.3f\"\n",
    "      #% metrics.adjusted_mutual_info_score(labels_true, labels))\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(X, labels))\n",
    "\n",
    "unique_labels = set(labels)\n",
    "colors = [plt.cm.Spectral(each)\n",
    "          for each in np.linspace(0, 1, len(unique_labels))]\n",
    "for k, col in zip(unique_labels, colors):\n",
    "    class_member_mask = (labels == k)\n",
    "    xy = X[class_member_mask & core_samples_mask]\n",
    "    plt.plot(xy[:, 0], xy[:, 1], 'o',markersize=5)\n",
    "    xy = X[class_member_mask & ~core_samples_mask]\n",
    "    plt.plot(xy[:, 0], xy[:, 1], 'o', markersize=6)\n",
    "plt.title('Estimated number of clusters: %d' % n_clusters_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#3: CLUSTERS OF DIFFERENT SIZES\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "centres = [[-1, 0], [1, 0], [3, 0]]\n",
    "\n",
    "X0, labels0_true = make_blobs(n_samples=100, centers=centres[0], cluster_std=[[0.2,0.2]])\n",
    "X1, labels1_true = make_blobs(n_samples=400, centers=centres[1], cluster_std=[[0.6,0.6]])\n",
    "X2, labels2_true = make_blobs(n_samples=100, centers=centres[2], cluster_std=[[0.2,0.2]])\n",
    "X = np.concatenate((X0,X1,X2))\n",
    "labels_true = np.concatenate((labels0_true,labels1_true+1,labels2_true+2))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.suptitle('Clusters of Different Sizes', fontsize=15)\n",
    "for k, col in zip(range(3), colors):\n",
    "    my_members = labels_true == k\n",
    "    cluster_center = centres[k]\n",
    "    plt.scatter(X[my_members, 0], X[my_members, 1], c=col, marker='o',s=20) \n",
    "    plt.scatter(cluster_center[0], cluster_center[1], c=col, marker='o', s=200)\n",
    "plt.axis('equal')\n",
    "plt.title('Original data')\n",
    "\n",
    "\n",
    "db = DBSCAN(eps=0.15, min_samples=5)\n",
    "db.fit(X)\n",
    "labels = db.labels_\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "\n",
    "# Plot result\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "\n",
    "print('Estimated number of clusters: %d' % n_clusters_)\n",
    "#print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels_true, labels))\n",
    "#print(\"Completeness: %0.3f\" % metrics.completeness_score(labels_true, labels))\n",
    "#print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels_true, labels))\n",
    "#print(\"Adjusted Rand Index: %0.3f\"\n",
    "     # % metrics.adjusted_rand_score(labels_true, labels))\n",
    "#print(\"Adjusted Mutual Information: %0.3f\"\n",
    "      #% metrics.adjusted_mutual_info_score(labels_true, labels))\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(X, labels))\n",
    "\n",
    "unique_labels = set(labels)\n",
    "colors = [plt.cm.Spectral(each)\n",
    "          for each in np.linspace(0, 1, len(unique_labels))]\n",
    "for k, col in zip(unique_labels, colors):\n",
    "    class_member_mask = (labels == k)\n",
    "    xy = X[class_member_mask & core_samples_mask]\n",
    "    plt.plot(xy[:, 0], xy[:, 1], 'o',markersize=5)\n",
    "    xy = X[class_member_mask & ~core_samples_mask]\n",
    "    plt.plot(xy[:, 0], xy[:, 1], 'o', markersize=6)\n",
    "plt.title('Estimated number of clusters: %d' % n_clusters_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other clustering options\n",
    "<img src=\"clustering_options.png\">\n",
    "<img src=\"clustering-differences.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 5 Lesson 1\n",
    "## Recommendation Engines\n",
    "9 - 20 June 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Recommendation engines aims to match users to things (movies, songs, items, events, etc) they might enjoy but have not yet tried. The rating is produced by analysing other user/item ratings (and sometimes item characteristics) to provide personalised recommendations to users.\n",
    "- There are two general approaches to the design:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content-based filtering\n",
    "- In content-based filtering, items are mapped into a feature space, and recommendations depend on item characteristics.\n",
    "- Content-based filtering begins by mapping each item into a feature space. Both users and items are represented by vectors in this space. \n",
    "- Item vectors measure the degree to which the item is described by each feature, and user vectors measure a user’s preferences for each feature.\n",
    "- Ratings are generated by taking dot products of user & item vectors.\n",
    "- Content-based filtering has some difficulties: \n",
    " - Must map items into a feature space (manual work)\n",
    " -  Recommendations are limited in scope (items must be similar to each other)\n",
    " - Hard to create cross-content recommendations (eg books/music films...this would require comparing elements from different feature spaces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"collaborative_filtering.png\" width=400 align=right>\n",
    "### Collaborative filtering\n",
    "- In contrast, the only data under consideration in collaborative filtering are user-item ratings, and recommendations depend on user preferences.\n",
    "- Collaborative filtering refers to a family of methods for predicting ratings where instead of thinking about users and items in terms of a feature space, we are only interested in the existing user-item ratings themselves.\n",
    "- In this case, our dataset is a ratings matrix whose columns correspond to items, and whose rows correspond to users. This will be the general form of the data we analyse for collaborative filtering. The method relies on previous user-item ratings (or feedback).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cold-start problem\n",
    "- Collaborative filtering is susceptible to the Cold Start problem.\n",
    "- What happens if we don’t have any (or enough) reviews?\n",
    "- Until users rate several items, we don’t know anything about their preferences.\n",
    "- We can get around this by enhancing our recommendations using implicit feedback, which may include things like item browsing behaviour, search patterns, purchase history, etc. Or by using a hybrid model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jaccard Similarity:\n",
    "- Defines similarity between two sets of objects.\n",
    "<img src=\"jacard-similarity.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measuring error\n",
    "<img src=\"measuring-error.png\" align=left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explicit vs Implicit Data\n",
    "- Explicit data is when you ask the user to rate something, e.g. 1-5 star rating for a movie.\n",
    "- Implicit data is when you observe a users behaviour and record."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithms for Recommendations\n",
    "- Alternating Least Squares (ALS) \n",
    "- Stochastic Gradient Descent (SGD) \n",
    "- Singular Value Decomposition (SVD) \n",
    "- Factorization Machine (FM) \n",
    "- Collaborative Less is More Filtering (CLiMF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install scikit-surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load Surprise\n",
    "from surprise import SVD\n",
    "from surprise import Dataset\n",
    "from surprise import evaluate, print_perf\n",
    "from surprise import Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reader = Reader(line_format='user item rating timestamp', sep='\\t')\n",
    "\n",
    "data = Dataset.load_from_file(file_path = '../data/u.data', reader=reader)\n",
    "data.split(n_folds=3)\n",
    "\n",
    "algo = SVD()\n",
    "perf = evaluate(algo, data, measures=['RMSE', 'MAE'])\n",
    "print_perf(perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 3. Make individual recommendations\n",
    "uid = str(196)  # raw user id (as in the ratings file). They are **strings**!\n",
    "iid = str(302)  # raw item id (as in the ratings file). They are **strings**!\n",
    "\n",
    "# get a prediction for specific users and items.\n",
    "pred = algo.predict(uid, iid, r_ui=2, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Other recommendation algorithms:\n",
    "#random_pred.NormalPredictor    Algorithm predicting a random rating based on the distribution of the training set, which is assumed to be normal.\n",
    "#baseline_only.BaselineOnly    Algorithm predicting the baseline estimate for given user and item.\n",
    "#knns.KNNBasic    A basic collaborative filtering algorithm.\n",
    "#knns.KNNWithMeans    A basic collaborative filtering algorithm, taking into account the mean ratings of each user.\n",
    "#knns.KNNBaseline    A basic collaborative filtering algorithm taking into account a baseline rating.\n",
    "#matrix_factorization.SVD    The famous SVD algorithm, as popularized by Simon Funk during the Netflix Prize.\n",
    "#matrix_factorization.SVDpp    The SVD++ algorithm, an extension of SVD taking into account implicit ratings.\n",
    "#matrix_factorization.NMF    A collaborative filtering algorithm based on Non-negative Matrix Factorization.\n",
    "#slope_one.SlopeOne    A simple yet accurate collaborative filtering algorithm.\n",
    "#co_clustering.CoClustering    A collaborative filtering algorithm based on co-clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 5 Lesson 2\n",
    "## Databases + SQL in Python\n",
    "10 - 22 June 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Databases\n",
    "- Databases are computer systems that manage storage and querying of data. Databases provide a way to organise data along with efficient methods to retrieve specific information.\n",
    "- Typically, retrieval is performed using a query language, a mini programming syntax with a few basic operators for data transformation, the most common of which is SQL.\n",
    "- Databases are the standard solution for data storage and are much more robust than text, CSV or json files. Most analyses involve pulling data to and from a resource and in most settings, that means using a database.\n",
    "- Databases can come in many flavours, designed to serve for different use cases.\n",
    "- Databases are ‘modelled’ to suit their intended purpose. \n",
    "\n",
    "#### Relational databases and schema\n",
    "- A relational database is a database based tabular data and links between data entities or concepts.\n",
    "- A relational database is organised into tables. Each table should correspond to one entity or concept.\n",
    "- A table is made up rows and columns, similar to a Pandas dataframe or Excel spreadsheet.\n",
    "- A table also has a schema which is a set of rules for what goes in each table. These specify what columns are contained in the table and what type those columns are (text, integers, floats, etc.).\n",
    "\n",
    "#### Primary and foreign keys\n",
    "- Each table typically has a primary key column. This column is a unique value per row and serves as the identifier for the row.\n",
    "- A table can have many foreign keys as well. A foreign key is a column that contains values to link the table to the other tables.\n",
    "\n",
    "#### Transactions\n",
    "- A unit of work performed against a database is called a transaction. This term generally represents any change in database.\n",
    "- ACID is a set of properties that guarantee that database transactions are processed reliably.\n",
    " - Atomicity \"all or nothing\": if one part of the transaction fails, the entire transaction fails, and the database state is left unchanged.\n",
    " - Consistency ensures that any transaction will bring the database from one valid state to another.\n",
    " - Isolation ensures that the concurrent execution of transactions results in a system state that would be obtained if transactions were executed serially, i.e., one after the other.\n",
    " - Durability ensures that once a transaction has been committed, it will remain so, even in the event of power loss, crashes, or errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The sqlite3 package\n",
    "- The command line utility can be useful for basic SQL tasks, but since we're using python for the rest of code it will often be easier to access sqlite directly from within python. We can use the python sqlite3 package for just this purpose.\n",
    "- Open a connection to an SQLite database file. As before, if the file does not already exist it will automatically be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "sqlite_db = 'test_db.sqlite'\n",
    "conn = sqlite3.connect(sqlite_db) \n",
    "c = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The syntax to create a table is similar to the console, only now we use the execute method of the cursor object c that we just created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c.execute('CREATE TABLE houses (field1 INTEGER PRIMARY KEY, sqft INTEGER, bdrms INTEGER, age INTEGER, price INTEGER);')\n",
    "\n",
    "# Save (commit) the changes\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the database saved the table should now be viewable using SQLite Manager.\n",
    "\n",
    "#### Adding data\n",
    "\n",
    "Since we're back in python, we can now use regular programming techniques in conjunction with the sqlite connection.  In particular, the cursor's `execute()` method supports value substitutionusing the `?` character, which makes adding multiple records a bit easier.  See the [docs](https://docs.python.org/2.7/library/sqlite3.html) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "last_sale = (None, 4000, 5, 22, 619000)\n",
    "c.execute('INSERT INTO houses VALUES (?,?,?,?,?)',last_sale)\n",
    "\n",
    "# Remember to commit the changes\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Notice that in this syntax we use the python None value, rather than NULL, to trigger SQLite to auto-increment the Primary Key.\n",
    "- There is a related cursor method executemany() which takes an array of tuples and loops through them, substituting one tuple at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recent_sales = [\n",
    "  (None, 2390, 4, 34, 319000),\n",
    "  (None, 1870, 3, 14, 289000),\n",
    "  (None, 1505, 3, 90, 269000),\n",
    "]\n",
    "\n",
    "c.executemany('INSERT INTO houses VALUES (?, ?, ?, ?, ?)', recent_sales)\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c.execute('select * from houses').fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding data from a csv file\n",
    "- Next let's load our housing.csv data into an array, and then INSERT those records into the database. In this example we'll use the numpy genfromtxt function to read the file and parse the contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy import genfromtxt\n",
    "\n",
    "# import into nparray of ints, then convert to list of lists\n",
    "data = genfromtxt('../data/housing-data.csv', dtype='i8', delimiter=',', skip_header=1).tolist()\n",
    "\n",
    "# append a None value to beginning of each sub-list\n",
    "for d in data:\n",
    "    d.insert(0, None)\n",
    "\n",
    "for d in data:\n",
    "    c.execute('INSERT INTO houses VALUES (?, ?, ?, ?, ?)', d)\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remember that all elements in a numpy array must be the same data type, so if we want to 'add a None' to each row, we need to work around this. Lists can contain mixed types, so that is one approach.\n",
    "- Still, in this case the value we're adding is the same for all records, so we could have simply used a 'None' in the INSERT statement directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deleting data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# similar syntax as before\n",
    "results = c.execute(\"SELECT * FROM houses WHERE bdrms = 4\")\n",
    "\n",
    "# here results is a cursor object - use fetchall() to extract a list\n",
    "results.fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas connector\n",
    "\n",
    "- While databases provide many analytical capabilities, often it's useful to pull the data back into Python for more flexible programming. Large, fixed operations would be more efficient in a database, but Pandas allows for interactive processing.\n",
    "- For example, if you want to aggregate nightly log-ins or sales to present a report or dashboard, this operation is likely not changing and operating on a large dataset. This can run very efficiently in a database rather than by connecting to it with Python.\n",
    "- However, if we want to investigate login or sales data further and ask more interactive questions, then Python would be more practical.\n",
    "- Pandas can connect to most relational databases. In this demonstration, we will create and connect to a SQLite database.\n",
    "- SQLite creates portable SQL databases saved in a single file. These databases are stored in a very efficient manner and allow fast querying, making them ideal for small databases or databases that need to be moved across machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.io import sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing data into a database\n",
    "\n",
    "Data in Pandas can be loaded into a relational database. For the most part, Pandas can use column information to infer the schema for the table it creates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('../data/housing-data.csv', low_memory=False)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is moved to the database through the to_sql command, similar to the to_csv command.\n",
    "to_sql takes as arguments:\n",
    "- `name`, the table name to create\n",
    "- `con`, a connection to a database\n",
    "- `index`, whether to input the index column\n",
    "- `schema`, if we want to write a custom schema for the new table\n",
    "- `if_exists`, what to do if the table already exists. We can overwrite it, add to it, or fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.to_sql('houses_pandas',\n",
    "            con=conn,\n",
    "            if_exists='replace',\n",
    "            index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c.execute('select bdrms, avg(price) from houses_pandas group by bdrms').fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"guided-practice\"></a>\n",
    "\n",
    "\n",
    "### SQL Syntax \n",
    "\n",
    "\n",
    "###### Select\n",
    "Every query should start with `SELECT`.  `SELECT` is followed by the names of the columns in the output.\n",
    "\n",
    "`SELECT` is always paired with `FROM`, and `FROM` identifies the table to retrieve data from.\n",
    "\n",
    "```sql\n",
    "SELECT\n",
    "<columns>\n",
    "FROM\n",
    "<table>\n",
    "```\n",
    "\n",
    "`SELECT *` denotes returns *all* of the columns.\n",
    "\n",
    "Housing Data example:\n",
    "```sql\n",
    "SELECT\n",
    "sqft, bdrms\n",
    "FROM houses_pandas;\n",
    "```\n",
    "\n",
    "**Check:** Write a query that returns the `sqft`, `bdrms` and `price`.\n",
    ">\n",
    "```sql\n",
    "SELECT\n",
    "sqft, bdrms, price\n",
    "FROM houses_pandas;\n",
    "```\n",
    "\n",
    "##### Where\n",
    "\n",
    "`WHERE` is used to filter table to a specific criteria and follows the `FROM` clause.\n",
    "\n",
    "```sql\n",
    "SELECT\n",
    "<columns>\n",
    "FROM\n",
    "<table>\n",
    "WHERE\n",
    "<condition>\n",
    "```\n",
    "Example:\n",
    "```sql\n",
    "SELECT\n",
    "sqft, bdrms, age, price\n",
    "FROM houses_pandas\n",
    "WHERE bdrms = 2 and price < 250000;\n",
    "```\n",
    "\n",
    "The condition is some filter applied to the rows, where rows that match the condition will be in the output.\n",
    "\n",
    "**Check:** Write a query that returns the `sqft`, `bdrms`, `age` for when houses older than 60 years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Aggregations\n",
    "\n",
    "Aggregations (or aggregate functions) are functions where the values of multiple rows are grouped together as input on certain criteria to form a single value of more significant meaning or measurement such as a set, a bag or a list.\n",
    "\n",
    "Examples of aggregate funtions:\n",
    "\n",
    "- Average (i.e., arithmetic mean)\n",
    "- Count\n",
    "- Maximum\n",
    "- Minimum\n",
    "- Median\n",
    "- Mode\n",
    "- Sum\n",
    "\n",
    "In SQL they are performed in a `SELECT` statement as follows.\n",
    "\n",
    "```sql\n",
    "SELECT COUNT(price)\n",
    "FROM houses_pandas;\n",
    "```\n",
    "\n",
    "```sql\n",
    "SELECT AVG(sqft), MIN(price), MAX(price)\n",
    "FROM houses_pandas\n",
    "WHERE bdrms = 2;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Joins\n",
    "\n",
    "Below is a link to a handy reference for SQL joins. In this chart joins are represented in terms of sets and venn diagrams. \n",
    "https://www.codeproject.com/Articles/33052/Visual-Representation-of-SQL-Joins\n",
    "\n",
    "Alternatively, remember the merge functionality of pandas.\n",
    "https://github.com/pandas-dev/pandas/blob/master/doc/cheatsheet/Pandas_Cheat_Sheet.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "conn = sqlite3.connect('../data/enron.db') \n",
    "c = conn.cursor()\n",
    "c.execute(\"SELECT name FROM sqlite_master WHERE type='table';\").fetchall()\n",
    "fields = c.execute(\"SELECT sql from sqlite_master WHERE type='table' and name='MessageBase';\").fetchall()\n",
    "print (''.join(fields[0]))\n",
    "fields = c.execute(\"SELECT sql from sqlite_master WHERE type='table' and name='RecipientBase';\").fetchall()\n",
    "print (''.join(fields[0]))\n",
    "fields = c.execute(\"SELECT sql from sqlite_master WHERE type='table' and name='EmployeeBase';\").fetchall()\n",
    "print (''.join(fields[0]))\n",
    "\n",
    "results = c.execute(\"SELECT * FROM EmployeeBase LIMIT 5;\").fetchall()\n",
    "for row in results:\n",
    "    print (row)\n",
    "\n",
    "import pandas as pd\n",
    "employees = pd.read_sql(\"SELECT * FROM EmployeeBase;\", conn)\n",
    "recipients = pd.read_sql(\"SELECT * FROM RecipientBase;\", conn)\n",
    "messages = pd.read_sql(\"SELECT * FROM MessageBase;\", conn)\n",
    "\n",
    "employees.describe()\n",
    "\n",
    "#representing data\n",
    "import scipy.special\n",
    "from bokeh.layouts import gridplot\n",
    "from bokeh.plotting import figure, output_notebook, show\n",
    "p1 = figure(title=\"Distribution of Recipients\",tools=\"save\")\n",
    "hist, edges = np.histogram(recipients.rno, density=False, bins=10)\n",
    "x = np.arange(0,57)\n",
    "p1.quad(top=hist, bottom=0, left=edges[:10], right=edges[1:], fill_color=\"#008080\")\n",
    "p1.legend.location = \"center_right\"\n",
    "p1.xaxis.axis_label = 'No of Recipients'\n",
    "p1.yaxis.axis_label = 'Messages'\n",
    "output_notebook()\n",
    "show(p1)\n",
    "\n",
    "#Pandas merge\n",
    "rm = pd.merge(recipients,messages,on=\"mid\")\n",
    "rme = pd.merge(rm,employees,left_on=\"from_eid\",right_on=\"eid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional resources\n",
    "- [sqlite3 home](http://www.sqlite.org)  \n",
    "- [SQLite - Python tutorial](http://sebastianraschka.com/Articles/2014_sqlite_in_python_tutorial.html)  \n",
    "- [SQL zoo](http://www.sqlzoo.net)  Great for learning syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 6 Lesson 1\n",
    "## Decision trees\n",
    "11 - 27 June 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 6 Lesson 2\n",
    "## Random forest\n",
    "12 - 29 June 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 7 Lesson 1\n",
    "## X\n",
    "13 - 4 July 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 7 Lesson 2\n",
    "## X\n",
    "14 - 6 July 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 8 Lesson 1:\n",
    "15 - 11 July 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 8 Lesson 2\n",
    "## X\n",
    "16 - 13 July 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 9 Lesson 1\n",
    "## X\n",
    "17 - 18 July 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 9 Lesson 2\n",
    "## X\n",
    "18 - 20 July 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 10 Lesson 1\n",
    "## X\n",
    "19 - 25 July 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 10 Lesson 2\n",
    "## X\n",
    "20 - 27 July 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
