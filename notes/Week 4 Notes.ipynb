{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 4 Lesson 1\n",
    "## Regularisation\n",
    "7 - 13 June 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset selection\n",
    "* We could fit a separate linear regression model for every combination of our features. But what happens when we have a large number of features? Computation time becomes a factor and we also need to consider that as we include more features we are increasing the chance we include a variable that doesn’t add any predictive power for future data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "crime = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/communities/communities.data', header=None, na_values=['?'])\n",
    "crime = crime.iloc[:, 5:]\n",
    "crime.dropna(inplace=True)\n",
    "crime.head()\n",
    "X = crime.iloc[:, :-1]\n",
    "y = crime.iloc[:, -1]\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lm = LinearRegression()\n",
    "lm.fit(X_train, y_train)\n",
    "lm.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularisation\n",
    "* Regularisation works by using a tuning parameter lambda or alpha that imposes a penalty on the size of coefficients.\n",
    "* Instead of minimizing the \"loss function\" (mean squared error), it minimizes the \"loss plus penalty\".\n",
    "* A tiny alpha imposes no penalty on the coefficient size, and is equivalent to a normal linear model.\n",
    "* Increasing the alpha penalizes the coefficients and shrinks them toward zero.\n",
    "\n",
    "* Recall from Week 2 that the ordinary least squares procedure estimates coefficients that minimise:\n",
    "<img src=\"RSS-equation-again.png\">\n",
    "* Regularization (or Shrinkage) is a way to constrain the estimates of beta to be close or equal to zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "preds = lm.predict(X_test)\n",
    "print('RMSE (no regularization) =', np.sqrt(metrics.mean_squared_error(y_test, preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge Regression regularisation\n",
    "* Ridge Regression is similar to least squares, except we include a penalty term, the λ term is a tuning parameter. \n",
    "* When it is zero we get least squares, as it increases the term, (the shrinkage penalty) has more of an impact and the coefficients will **approach** zero.\n",
    "<img src=\"img/ridge-regression.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "rreg = Ridge(alpha=0.1, normalize=True)\n",
    "rreg.fit(X_train, y_train)\n",
    "rreg.coef_\n",
    "preds = rreg.predict(X_test)\n",
    "print('RMSE (Ridge reg.) =', np.sqrt(metrics.mean_squared_error(y_test, preds)))\n",
    "# Is this model better? Why?\n",
    "# Yes, this model is a better result. It is removing those coefficients / columns that don't play a part in predicting the value of X.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use RidgeCV to select best alpha\n",
    "from sklearn.linear_model import RidgeCV\n",
    "alpha_range = 10.**np.arange(-2, 3)\n",
    "rregcv = RidgeCV(normalize=True, scoring='neg_mean_squared_error', alphas=alpha_range)\n",
    "rregcv.fit(X_train, y_train)\n",
    "# Print the optimal value of Alpha for Ridge Regression\n",
    "print('Optimal Alpha Value: ', rregcv.alpha_)\n",
    "# Print the RMSE for the ridge regression model\n",
    "preds = rregcv.predict(X_test)\n",
    "print ('RMSE (Ridge CV reg.) =', np.sqrt(metrics.mean_squared_error(y_test, preds)))\n",
    "# What is the range of alpha values we are searching over?\n",
    "# We are searching from 0.01 (10^-2) and 1000 (10^3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lasso Regression regularisation\n",
    "Lasso Regression is similar to Ridge Regression, except we have the absolute value of beta in our penalty term, the λ term is a tuning parameter. \n",
    "When it is zero we get least squares, as it increases the term, (the shrinkage penalty) has more of an impact and the coefficients will **equal** zero.\n",
    "<img src=\"img/lasso-regression.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/comparelassoridge.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Lasso regularization is useful if we believe many features are irrelevant, since a feature with a zero coefficient is essentially removed from the model. Thus, it is a useful technique for feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "########## Lasso Regression Model ##########\n",
    "# lasso (alpha must be positive, larger means more regularization)\n",
    "from sklearn.linear_model import Lasso\n",
    "las = Lasso(alpha=0.01, normalize=True)\n",
    "las.fit(X_train, y_train)\n",
    "las.coef_\n",
    "preds = las.predict(X_test)\n",
    "print('RMSE (Lasso reg.) =', np.sqrt(metrics.mean_squared_error(y_test, preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use LassoCV to select best alpha (tries 100 alphas by default)\n",
    "from sklearn.linear_model import LassoCV\n",
    "alpha_range = 10.**np.arange(-10, 10)\n",
    "lascv = LassoCV(normalize=True, alphas=alpha_range)\n",
    "lascv.fit(X_train, y_train)\n",
    "print('Optimal Alpha Value: ',lascv.alpha_)\n",
    "lascv.coef_\n",
    "preds = lascv.predict(X_test)\n",
    "print('RMSE (Lasso CV reg.) =', np.sqrt(metrics.mean_squared_error(y_test, preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "enet = ElasticNet(alpha=0.1, l1_ratio=0.7)\n",
    "enet.fit(X_train, y_train)\n",
    "enet.coef_\n",
    "preds=enet.predict(X_test)\n",
    "print('RMSE (ENET reg.)', np.sqrt(metrics.mean_squared_error(y_test, preds)))\n",
    "print('-----')\n",
    "\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "enetCV = ElasticNetCV(normalize=True, alphas=alpha_range)\n",
    "enetCV.fit(X_train, y_train)\n",
    "print('Optimal Alpha Value: ',enetCV.alpha_)\n",
    "enetCV.coef_\n",
    "preds = enetCV.predict(X_test)\n",
    "print('RMSE (ENET CV reg.)', np.sqrt(metrics.mean_squared_error(y_test, preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 4 Lesson 2\n",
    "## Clustering\n",
    "8 - 15 June 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is clustering?\n",
    "* Recall unsupervised learning is when we are trying to find interesting patterns or groups in our data. We don’t have a variable we are trying to predict (a Y value).\n",
    "* Clustering aims to discover subgroups in our data where the points are similar to each other. So we have a collection of groups and all points belonging to the same group are similar. Points in different groups are different to each other.\n",
    "* We have to decide what variables we will construct the groups on. What makes them different (or similar)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why do we cluster data?\n",
    "* To enhance our understanding of a dataset by dividing the data into groups.\n",
    "* Clustering provides a layer of abstraction from individual data points.\n",
    "* The goal is to extract and enhance the natural structure of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we cluster data?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-means clustering\n",
    "1. Choose k initial centroids (note that k is an input):\n",
    " * randomly (but may yield divergent behavior)\n",
    " * perform alternative clustering task, use resulting centroids as initial k-means centroids\n",
    " * start with global centroid, choose point at max distance, repeat (but might select outlier)\n",
    "2. For each point:\n",
    " * find distance to each centroid\n",
    " * assign point to nearest centroid:\n",
    "  * The similarity criterion is determined by the measure we choose.\n",
    "  * In the case of k-means clustering, the similarity metric is the Euclidian distance:\n",
    "  <img src=\"img/euclidian-distance.png\">\n",
    "3. Recalculate centroid positions.\n",
    "4. Repeat steps 2-3 until stopping criteria met.\n",
    " * We iterate until some stopping criteria are met; in general, suitable convergence is achieved in a small number of steps. \n",
    " * Stopping criteria can be based on the centroids (eg, if positions change by no more than ε) or on the points (eg, if no more than x% change clusters between iterations).\n",
    " \n",
    "See this great visualisation: http://shabal.in/visuals/kmeans/6.html\n",
    "\n",
    "* Strengths: K-means is a popular algorithm because of its computational efficiency and simple and intuitive nature.\n",
    "* Weaknesses: However, K-means is highly scale dependent, and is not suitable for data with widely varying shapes and densities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "d = iris.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "est = KMeans(n_clusters=3, init='random')\n",
    "est.fit(d)\n",
    "y_kmeans = est.predict(d)\n",
    "#plot it up with centres\n",
    "colors = np.array(['#FF0054','#FBD039','#23C2BC'])\n",
    "plt.figure()\n",
    "plt.scatter(d[:, 2], d[:, 0], c=colors[y_kmeans], s=50)\n",
    "plt.xlabel(iris.feature_names[2])\n",
    "plt.ylabel(iris.feature_names[0])\n",
    "plt.scatter(centers[:, 2], centers[:, 0], c='k', linewidths=3,\n",
    "            marker='+', s=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster validation (the Silhouette Coefficient)\n",
    "* Cohesion measures clustering effectiveness within a cluster.\n",
    "<img src=\"img/cohesion.png\">\n",
    "* Separation measures clustering effectiveness between clusters.\n",
    "<img src=\"img/separation.png\">\n",
    "* A useful measure that combines the ideas of cohesion and separation is the silhouette coefficient. For point xi, this is given by: \n",
    "<img src=\"img/silhouette-coefficient.png\">\n",
    "* such that: \n",
    " * ai = average in-cluster distance to xi\n",
    " * bij = average between-cluster distance to xi\n",
    " * bi = minj(bij).\n",
    "* The silhouette coefficient can take values between -1 and 1.\n",
    "* In general, we want separation to be high and cohesion to be low. \n",
    "* This corresponds to a value of SC close to +1. A negative silhouette coefficient means the cluster radius is larger than the space between clusters, and thus clusters overlap\n",
    "* One useful application of cluster validation and the silhouette coefficient is to determine the best number of clusters for your dataset.  \n",
    "* Ultimately, cluster validation and clustering in general are suggestive techniques that rely on human interpretation to be meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "'''\n",
    "K = 2\n",
    "'''\n",
    "\n",
    "# Generate and Plot Dummy Data for k = 2\n",
    "centres = [[2, 0.75], [0, 0]]\n",
    "X0, labels0_true = make_blobs(n_samples=300, centers=centres[0], cluster_std=[[0.2,0.2]])\n",
    "X1, labels1_true = make_blobs(n_samples=300, centers=centres[1], cluster_std=[[0.2,0.2]])\n",
    "X = np.concatenate((X0,X1))\n",
    "labels_true = np.concatenate((labels0_true,labels1_true+1))\n",
    "colors = np.array(['#FF0054','#FBD039'])\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.suptitle('Cluster Validation Evaluation', fontsize=15)\n",
    "plt.subplot(331)\n",
    "plt.text(-0.5, 1.5, 'k=2', fontsize=14)\n",
    "for k, col in zip(range(2), colors):\n",
    "    my_members = labels_true == k\n",
    "    cluster_center = centres[k]\n",
    "    plt.scatter(X[my_members, 0], X[my_members, 1], c=col, marker='o',s=20) \n",
    "    plt.scatter(cluster_center[0], cluster_center[1], c=col, marker='o', s=200)\n",
    "plt.axis('equal')\n",
    "plt.title('Data with truth labels')\n",
    "plt.ylabel('y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k_rng = range(1,15)\n",
    "est = [KMeans(n_clusters = k).fit(X) for k in k_rng]\n",
    "silhouette_score = [metrics.silhouette_score(X, e.labels_, metric='euclidean') for e in est[1:]]\n",
    "plt.plot(k_rng[1:], silhouette_score, 'b*-')\n",
    "plt.xlim([1,15])\n",
    "plt.grid(True)\n",
    "plt.title('Silhouette Coefficient')\n",
    "plt.plot(2,silhouette_score[0], 'o', markersize=12, markeredgewidth=1.5,\n",
    "         markerfacecolor='None', markeredgecolor='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate the within sum of squared errors for different values of K, highlighting k = 2\n",
    "within_sum_squares = [e.inertia_ for e in est]\n",
    "plt.plot(k_rng, within_sum_squares, 'b*-')\n",
    "plt.xlim([1,15])\n",
    "plt.grid(True)\n",
    "plt.title('Within Sum of Squared Errors')\n",
    "plt.plot(2,within_sum_squares[1], 'ro', markersize=12, markeredgewidth=1.5,\n",
    "         markerfacecolor='None', markeredgecolor='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limitations of K-Means\n",
    "\n",
    "Adapted from Bart Baddely's 2014 PyData Presentation: http://nbviewer.ipython.org/github/BartBaddeley/PyDataTalk-2014/blob/master/PyDataTalk.ipynb\n",
    "\n",
    "Agenda: \n",
    "1. K-means might not work when dimensions have different scales\n",
    "2. K-means might not work for non-spherical shapes\n",
    "3. K-means might not work for clusters of different sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "from sklearn.datasets.samples_generator import make_blobs, make_moons\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#1) DIMENSIONS WITH DIFFERENT SCALES\n",
    "\n",
    "# Generate data with differing variances\n",
    "np.random.seed(0)\n",
    "\n",
    "centres = [[1, 0.75], [1, -0.75], [0, 0]]\n",
    "\n",
    "X0, labels0_true = make_blobs(n_samples=300, centers=centres[0], cluster_std=[[0.6,0.1]])\n",
    "X1, labels1_true = make_blobs(n_samples=300, centers=centres[1], cluster_std=[[0.6,0.1]])\n",
    "X2, labels2_true = make_blobs(n_samples=300, centers=centres[2], cluster_std=[[0.6,0.1]])\n",
    "X = np.concatenate((X0,X1,X2))\n",
    "labels_true = np.concatenate((labels0_true,labels1_true+1,labels2_true+2))\n",
    "\n",
    "colors = np.array(['#FF0054','#FBD039','#23C2BC'])\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.suptitle('Dimensions with Different Scales', fontsize=15)\n",
    "for k, col in zip(range(3), colors):\n",
    "    my_members = labels_true == k\n",
    "    cluster_center = centres[k]\n",
    "    plt.scatter(X[my_members, 0], X[my_members, 1], c=col, marker='o',s=20) \n",
    "    plt.scatter(cluster_center[0], cluster_center[1], c=col, marker='o', s=200)\n",
    "plt.axis('equal')\n",
    "plt.title('Original data')\n",
    "\n",
    "# Compute clustering with 3 Clusters\n",
    "k_means_3 = KMeans(init='k-means++', n_clusters=3, n_init=10)\n",
    "k_means_3.fit(X)\n",
    "k_means_3_labels = k_means_3.labels_\n",
    "k_means_3_cluster_centres = k_means_3.cluster_centers_\n",
    "\n",
    "# Plot result\n",
    "distance = euclidean_distances(k_means_3_cluster_centres,\n",
    "                               centres,\n",
    "                               squared=True)\n",
    "order = distance.argmin(axis=0)\n",
    "for k, col in zip(range(3), colors):              \n",
    "    my_members = k_means_3_labels == order[k]\n",
    "    plt.scatter(X[my_members, 0], X[my_members, 1],c=col, marker='o', s=20)           \n",
    "    cluster_center = k_means_3_cluster_centres[order[k]]\n",
    "    plt.scatter(cluster_center[0], cluster_center[1], marker = 'o', c=col, s=200, alpha=0.8)            \n",
    "plt.axis('equal')\n",
    "plt.title('KMeans 3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#2: NON-SPHERICAL SHAPES\n",
    "\n",
    "[X, true_labels] = make_moons(n_samples=1000, noise=.05)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.suptitle('Non-Spherical Shapes', fontsize=15)\n",
    "for k, col in zip(range(2), colors):\n",
    "    my_members = true_labels == k\n",
    "    plt.scatter(X[my_members, 0], X[my_members, 1], c=col, marker='o', s=20)\n",
    "\n",
    "plt.axis('equal')\n",
    "plt.title('Original Data') \n",
    "    \n",
    "# Compute clustering with 2 Clusters\n",
    "k_means_2 = KMeans(init='k-means++', n_clusters=2, n_init=10)\n",
    "k_means_2.fit(X)\n",
    "k_means_2_labels = k_means_2.labels_\n",
    "k_means_2_cluster_centers = k_means_2.cluster_centers_\n",
    "\n",
    "for k, col in zip(range(2), colors):           \n",
    "    my_members = k_means_2_labels == k\n",
    "    plt.scatter(X[my_members, 0], X[my_members, 1],c=col, marker='o', s=20)     \n",
    "    cluster_center = k_means_2_cluster_centers[k]\n",
    "    plt.scatter(cluster_center[0], cluster_center[1], marker = 'o', c=col, s=200, alpha=0.8) \n",
    "plt.axis('equal')\n",
    "plt.title('KMeans 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#3: CLUSTERS OF DIFFERENT SIZES\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "centres = [[-1, 0], [1, 0], [3, 0]]\n",
    "\n",
    "X0, labels0_true = make_blobs(n_samples=100, centers=centres[0], cluster_std=[[0.2,0.2]])\n",
    "X1, labels1_true = make_blobs(n_samples=400, centers=centres[1], cluster_std=[[0.6,0.6]])\n",
    "X2, labels2_true = make_blobs(n_samples=100, centers=centres[2], cluster_std=[[0.2,0.2]])\n",
    "X = np.concatenate((X0,X1,X2))\n",
    "labels_true = np.concatenate((labels0_true,labels1_true+1,labels2_true+2))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.suptitle('Clusters of Different Sizes', fontsize=15)\n",
    "for k, col in zip(range(3), colors):\n",
    "    my_members = labels_true == k\n",
    "    cluster_center = centres[k]\n",
    "    plt.scatter(X[my_members, 0], X[my_members, 1], c=col, marker='o',s=20) \n",
    "    plt.scatter(cluster_center[0], cluster_center[1], c=col, marker='o', s=200)\n",
    "plt.axis('equal')\n",
    "plt.title('Original data')\n",
    "\n",
    "# Compute clustering with 3 Clusters\n",
    "k_means_3 = KMeans(init='k-means++', n_clusters=3, n_init=10)\n",
    "k_means_3.fit(X)\n",
    "k_means_3_labels = k_means_3.labels_\n",
    "k_means_3_cluster_centres = k_means_3.cluster_centers_\n",
    "\n",
    "# Plot result\n",
    "distance = euclidean_distances(k_means_3_cluster_centres,\n",
    "                               centres,\n",
    "                               squared=True)\n",
    "order = distance.argmin(axis=0)\n",
    "for k, col in zip(range(3), colors):              \n",
    "    my_members = k_means_3_labels == order[k]\n",
    "    plt.scatter(X[my_members, 0], X[my_members, 1],c=col, marker='o', s=20)           \n",
    "    cluster_center = k_means_3_cluster_centres[order[k]]\n",
    "    plt.scatter(cluster_center[0], cluster_center[1], marker = 'o', c=col, s=200, alpha=0.8)            \n",
    "plt.axis('equal')\n",
    "plt.title('KMeans 3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DBSCAN clustering\n",
    "\n",
    "* Criteria:\n",
    " * ε (or Epsilon) is the radius \n",
    " * minPoints (number of points within the ε-Neighborhood required for classification)\n",
    "* DBSCAN iterates through every point\n",
    "* Core object is a point meeting the criteria.\n",
    "* Outlier is outside the radius.\n",
    "* Pros\n",
    " * Recovers more complex cluster shapes\n",
    " * Finds the number of clusters\n",
    " * Automatically find outliers\n",
    "* Cons\n",
    " * Requires a distance function\n",
    " * Not as scalable as K-means\n",
    " * Calculating connected components can be difficult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.datasets.samples_generator import make_blobs, make_moons\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1) DIMENSIONS WITH DIFFERENT SCALES\n",
    "# Generate data with differing variances\n",
    "np.random.seed(0)\n",
    "\n",
    "centres = [[1, 0.75], [1, -0.75], [0, 0]]\n",
    "\n",
    "X0, labels0_true = make_blobs(n_samples=300, centers=centres[0], cluster_std=[[0.6,0.1]])\n",
    "X1, labels1_true = make_blobs(n_samples=300, centers=centres[1], cluster_std=[[0.6,0.1]])\n",
    "X2, labels2_true = make_blobs(n_samples=300, centers=centres[2], cluster_std=[[0.6,0.1]])\n",
    "X = np.concatenate((X0,X1,X2))\n",
    "labels_true = np.concatenate((labels0_true,labels1_true+1,labels2_true+2))\n",
    "\n",
    "colors = np.array(['#FF0054','#FBD039','#23C2BC'])\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.suptitle('Dimensions with Different Scales', fontsize=15)\n",
    "for k, col in zip(range(3), colors):\n",
    "    my_members = labels_true == k\n",
    "    cluster_center = centres[k]\n",
    "    plt.scatter(X[my_members, 0], X[my_members, 1], c=col, marker='o',s=20) \n",
    "    plt.scatter(cluster_center[0], cluster_center[1], c=col, marker='o', s=200)\n",
    "plt.axis('equal')\n",
    "plt.title('Original data')\n",
    "\n",
    "# Compute clustering with 3 Clusters\n",
    "db = DBSCAN(eps=0.2, min_samples=5)\n",
    "db.fit(X)\n",
    "labels = db.labels_\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "\n",
    "db_csi = db.core_sample_indices_\n",
    "\n",
    "# Plot result\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "\n",
    "print('Estimated number of clusters: %d' % n_clusters_)\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels_true, labels))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels_true, labels))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels_true, labels))\n",
    "print(\"Adjusted Rand Index: %0.3f\"\n",
    "      % metrics.adjusted_rand_score(labels_true, labels))\n",
    "print(\"Adjusted Mutual Information: %0.3f\"\n",
    "      % metrics.adjusted_mutual_info_score(labels_true, labels))\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(X, labels))\n",
    "\n",
    "unique_labels = set(labels)\n",
    "colors = [plt.cm.Spectral(each)\n",
    "          for each in np.linspace(0, 1, len(unique_labels))]\n",
    "for k, col in zip(unique_labels, colors):\n",
    "    class_member_mask = (labels == k)\n",
    "    xy = X[class_member_mask & core_samples_mask]\n",
    "    plt.plot(xy[:, 0], xy[:, 1], 'o',markersize=5)\n",
    "    xy = X[class_member_mask & ~core_samples_mask]\n",
    "    plt.plot(xy[:, 0], xy[:, 1], 'o', markersize=6)\n",
    "plt.title('Estimated number of clusters: %d' % n_clusters_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#2: NON-SPHERICAL SHAPES\n",
    "\n",
    "[X, true_labels] = make_moons(n_samples=1000, noise=.05)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.suptitle('Non-Spherical Shapes', fontsize=15)\n",
    "for k, col in zip(range(2), colors):\n",
    "    my_members = true_labels == k\n",
    "    plt.scatter(X[my_members, 0], X[my_members, 1], c=col, marker='o', s=20)\n",
    "\n",
    "plt.axis('equal')\n",
    "plt.title('Original Data') \n",
    "    \n",
    "db = DBSCAN(eps=0.2, min_samples=5)\n",
    "db.fit(X)\n",
    "labels = db.labels_\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "\n",
    "# Plot result\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "\n",
    "print('Estimated number of clusters: %d' % n_clusters_)\n",
    "#print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels_true, labels))\n",
    "#print(\"Completeness: %0.3f\" % metrics.completeness_score(labels_true, labels))\n",
    "#print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels_true, labels))\n",
    "#print(\"Adjusted Rand Index: %0.3f\"\n",
    "     # % metrics.adjusted_rand_score(labels_true, labels))\n",
    "#print(\"Adjusted Mutual Information: %0.3f\"\n",
    "      #% metrics.adjusted_mutual_info_score(labels_true, labels))\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(X, labels))\n",
    "\n",
    "unique_labels = set(labels)\n",
    "colors = [plt.cm.Spectral(each)\n",
    "          for each in np.linspace(0, 1, len(unique_labels))]\n",
    "for k, col in zip(unique_labels, colors):\n",
    "    class_member_mask = (labels == k)\n",
    "    xy = X[class_member_mask & core_samples_mask]\n",
    "    plt.plot(xy[:, 0], xy[:, 1], 'o',markersize=5)\n",
    "    xy = X[class_member_mask & ~core_samples_mask]\n",
    "    plt.plot(xy[:, 0], xy[:, 1], 'o', markersize=6)\n",
    "plt.title('Estimated number of clusters: %d' % n_clusters_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#3: CLUSTERS OF DIFFERENT SIZES\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "centres = [[-1, 0], [1, 0], [3, 0]]\n",
    "\n",
    "X0, labels0_true = make_blobs(n_samples=100, centers=centres[0], cluster_std=[[0.2,0.2]])\n",
    "X1, labels1_true = make_blobs(n_samples=400, centers=centres[1], cluster_std=[[0.6,0.6]])\n",
    "X2, labels2_true = make_blobs(n_samples=100, centers=centres[2], cluster_std=[[0.2,0.2]])\n",
    "X = np.concatenate((X0,X1,X2))\n",
    "labels_true = np.concatenate((labels0_true,labels1_true+1,labels2_true+2))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.suptitle('Clusters of Different Sizes', fontsize=15)\n",
    "for k, col in zip(range(3), colors):\n",
    "    my_members = labels_true == k\n",
    "    cluster_center = centres[k]\n",
    "    plt.scatter(X[my_members, 0], X[my_members, 1], c=col, marker='o',s=20) \n",
    "    plt.scatter(cluster_center[0], cluster_center[1], c=col, marker='o', s=200)\n",
    "plt.axis('equal')\n",
    "plt.title('Original data')\n",
    "\n",
    "\n",
    "db = DBSCAN(eps=0.15, min_samples=5)\n",
    "db.fit(X)\n",
    "labels = db.labels_\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "\n",
    "# Plot result\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "\n",
    "print('Estimated number of clusters: %d' % n_clusters_)\n",
    "#print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels_true, labels))\n",
    "#print(\"Completeness: %0.3f\" % metrics.completeness_score(labels_true, labels))\n",
    "#print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels_true, labels))\n",
    "#print(\"Adjusted Rand Index: %0.3f\"\n",
    "     # % metrics.adjusted_rand_score(labels_true, labels))\n",
    "#print(\"Adjusted Mutual Information: %0.3f\"\n",
    "      #% metrics.adjusted_mutual_info_score(labels_true, labels))\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(X, labels))\n",
    "\n",
    "unique_labels = set(labels)\n",
    "colors = [plt.cm.Spectral(each)\n",
    "          for each in np.linspace(0, 1, len(unique_labels))]\n",
    "for k, col in zip(unique_labels, colors):\n",
    "    class_member_mask = (labels == k)\n",
    "    xy = X[class_member_mask & core_samples_mask]\n",
    "    plt.plot(xy[:, 0], xy[:, 1], 'o',markersize=5)\n",
    "    xy = X[class_member_mask & ~core_samples_mask]\n",
    "    plt.plot(xy[:, 0], xy[:, 1], 'o', markersize=6)\n",
    "plt.title('Estimated number of clusters: %d' % n_clusters_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other clustering options\n",
    "<img src=\"img/clustering_options.png\">\n",
    "<img src=\"img/clustering-differences.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
